"""
Detect the timestamp of a MAJOR disturbance in a repeating signal pattern (shape-only, strict).

Why this should fix early detections (e.g., 4.70 vs true 4.81):
- Shape-only: every window is z-normalized (offset/scale removed).
- Phase-invariant: uses circular correlation, robust to small timing jitter.
- Strong persistence requirement: the low-correlation state must persist
  for ~2 full periods (not just a couple of bad windows).
- Pre-change stability: requires at least ~1 period of high, stable correlation
  right before the change, eliminating early transient dips.

Public API (drop-in):
- pattern_major_fault_time(signal_view) -> float
  signal_view: .x (timestamps), .y (values) OR (x, y) tuple
  returns: first timestamp (seconds) of a major pattern disturbance, or float("nan")
"""

from typing import Tuple, List
import numpy as np

# ------------------------------ Utilities ---------------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)
    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")
    # Drop NaNs, synchronized
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]
    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]
    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _median_filter(v: np.ndarray, k: int = 5) -> np.ndarray:
    if k <= 1 or v.size == 0:
        return v.copy()
    if k % 2 == 0:
        k += 1
    r = k // 2
    pad = np.pad(v, (r, r), mode="reflect")
    out = np.empty_like(v)
    for i in range(v.size):
        out[i] = np.median(pad[i:i+k])
    return out

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

# ---------------------------- Period estimation ----------------------------- #

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0
    fmin = 3.0 / (N * dt)
    fmax = 0.45 / dt
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    k = int(np.argmax(mag[band]))
    idx = np.flatnonzero(band)[k]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

# --------------------------- Shape-only template ---------------------------- #

def _resample_unit(seg: np.ndarray, L: int) -> np.ndarray:
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    z = seg - np.mean(seg)
    s = float(np.std(seg))
    if s > 0:
        z = z / s
    else:
        z = z * 0.0
    return np.interp(np.linspace(0, 1, L, endpoint=False), xi, z)

def _phase_invariant_corr(a: np.ndarray, b: np.ndarray) -> float:
    # a, b already z-normalized shapes in the same length space
    na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))
    if na == 0.0 or nb == 0.0:
        return 0.0
    a = a / na; b = b / nb
    c = np.fft.ifft(np.fft.fft(a) * np.conj(np.fft.fft(b))).real
    return float(np.clip(np.max(c), -1.0, 1.0))

def _template_from_learning(yu: np.ndarray, spp: int, L: int, learn_samples: int) -> np.ndarray:
    # Build windows over the learning region and keep the most consistent ones
    hop = max(1, spp // 8)
    windows = []
    for i in range(0, learn_samples - spp, hop):
        windows.append(_resample_unit(yu[i:i+spp], L))
    if len(windows) < 6:
        return np.array([], dtype=float)
    W = np.stack(windows, axis=0)
    # Initial template = mean
    tpl = np.mean(W, axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    # Score windows by correlation to initial template
    scores = np.array([_phase_invariant_corr(w, tpl) for w in W], dtype=float)
    # Keep top K consistent windows
    order = np.argsort(-scores)
    K = min(16, max(8, len(order)//2))
    top = W[order[:K]]
    tpl = np.mean(top, axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    return tpl

# ------------------------------- Detection ---------------------------------- #

def _correlation_series(tu: np.ndarray, yu: np.ndarray, spp: int, L: int, template: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    hop = max(1, spp // 8)  # ~8 hops per period
    times = []
    corr = []
    N = yu.size
    for i in range(0, N - spp, hop):
        seg = yu[i:i+spp]
        z = _resample_unit(seg, L)
        c = _phase_invariant_corr(z, template)
        times.append(tu[i + spp // 2])
        corr.append(c)
    return np.asarray(times, dtype=float), np.asarray(corr, dtype=float)

def _interpolated_first_drop_strict(times: np.ndarray, corrs: np.ndarray, learn_end_time: float, mc: float, sc: float, spp: int, hop: int) -> float:
    # Threshold: big drop and also low absolute correlation
    thr = min(mc - 5.0 * sc, 0.40)
    # Persistence: require ~2 periods below threshold
    hold_after = max(4, int(round(2.0 * spp / hop)))
    # Pre-stability: require ~1 period with high correlation before the change
    pre_ok_windows = max(3, int(round(1.0 * spp / hop)))
    pre_thr = max(0.70, mc - 1.0 * sc)

    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    below = corrs < thr

    # Scan for a run of low correlation preceded by high/stable correlation
    run = 0
    for j in range(j0, len(times)):
        if below[j]:
            run += 1
        else:
            run = 0
        if run >= hold_after:
            s = j - run + 1  # start index of low-corr run
            # Check pre-stability
            p0 = max(0, s - pre_ok_windows)
            if p0 < s and np.all(corrs[p0:s] > pre_thr):
                # Interpolate first threshold crossing between last high and first low
                k = s - 1
                while k >= j0 and below[k]:
                    k -= 1
                if k < j0:
                    return float(times[s])
                y0, y1 = float(corrs[k]), float(corrs[k+1])
                t0, t1 = float(times[k]), float(times[k+1])
                if y1 == y0 or t1 == t0:
                    return float(times[s])
                alpha = (thr - y0) / (y1 - y0)
                alpha = float(np.clip(alpha, 0.0, 1.0))
                return float(t0 + alpha * (t1 - t0))
    return float("nan")

# --------------------------------- Main ------------------------------------- #

def pattern_major_fault_time(signal_view) -> float:
    x, y = _as_numpy_xy(signal_view)
    if x.size < 256:
        return float("nan")

    # 1) Uniform resample
    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 256:
        return float("nan")

    # 2) Learn window (~30% but max half)
    N = tu.size
    N_learn = min(max(256, int(0.30 * N)), max(256, N // 2))
    learn_end_time = float(tu[min(N_learn - 1, N - 1)])
    yL = yu[:N_learn]

    # 3) Period estimate on learn window
    T = _best_period(yL, dt)
    if not np.isfinite(T):
        return float("nan")
    spp = int(round(T / dt))
    if spp < 12 or spp > N_learn // 2:
        return float("nan")

    # 4) Shape-only template from the learning region (window-based)
    L = 128
    template = _template_from_learning(yu, spp, L, learn_samples=N_learn)
    if template.size == 0:
        return float("nan")

    # 5) Correlation time series across the full record
    times, corr = _correlation_series(tu, yu, spp, L, template)
    if times.size == 0:
        return float("nan")
    corr_s = _median_filter(corr, k=5)  # suppress minute blips

    # 6) Baseline stats from windows fully inside the learn region
    learn_mask = times <= learn_end_time
    if not np.any(learn_mask):
        return float("nan")
    mc, sc = _robust_mean_std(corr_s[learn_mask])
    if not np.isfinite(mc) or not np.isfinite(sc):
        return float("nan")

    # 7) Strict drop with pre-stability and long persistence
    hop = max(1, spp // 8)
    t = _interpolated_first_drop_strict(times, corr_s, learn_end_time, mc, sc, spp, hop)
    return float(t)

__all__ = ["pattern_major_fault_time"]
