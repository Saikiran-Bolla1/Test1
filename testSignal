"""
Detect the timestamp of a MAJOR disturbance in a repeating signal pattern (shape-only).

Goal
- Detect the earliest time where the waveform's repeating SHAPE changes.
- Ignore minute changes in offset and amplitude (z-normalized comparisons).
- Robust to small timing jitter (phase-invariant correlation).
- Return a single timestamp (float seconds) or float("nan") if not detected.

How it works
1) Uniformly resample the signal.
2) Estimate the dominant period T (FFT + ACF fallback).
3) Segment the signal into cycles using peak picking with a minimum spacing ~ T.
4) Learn a shape-only template from an automatically selected stable set
   of the earliest cycles (high mutual correlation).
5) Score each subsequent cycle by phase-invariant, offset/scale-invariant correlation.
6) Find the first cycle whose correlation falls below a strict threshold (>=5Ïƒ drop and <0.45).
7) Refine the timestamp within that cycle:
   - Align the changed cycle to the template.
   - Slide a short window (~1/6 period) and find the earliest window where
     local shape-correlation drops below 0.55 for a few consecutive steps.
   - Return that sub-cycle time; if refinement fails, return the cycle start time.

No user-tunable parameters are exposed.
"""

from typing import Tuple, List
import numpy as np

# ------------------------------ Utilities ---------------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)
    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")
    # Drop NaNs, synchronized
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]
    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]
    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

def _median_filter(v: np.ndarray, k: int = 5) -> np.ndarray:
    if k <= 1 or v.size == 0:
        return v.copy()
    if k % 2 == 0:
        k += 1
    r = k // 2
    pad = np.pad(v, (r, r), mode="reflect")
    out = np.empty_like(v)
    for i in range(v.size):
        out[i] = np.median(pad[i:i+k])
    return out

# ---------------------------- Period estimation ----------------------------- #

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0
    fmin = 3.0 / (N * dt)
    fmax = 0.45 / dt
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    k = int(np.argmax(mag[band]))
    idx = np.flatnonzero(band)[k]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

# ----------------------------- Peak detection ------------------------------- #

def _find_local_maxima(y: np.ndarray) -> np.ndarray:
    # Simple local maxima (strict), handle flat-tops by picking the first flat index
    n = y.size
    if n < 3:
        return np.empty(0, dtype=int)
    dy1 = y[1:-1] - y[:-2]
    dy2 = y[2:] - y[1:-1]
    candidates = np.where((dy1 > 0) & (dy2 <= 0))[0] + 1
    # Include flat tops: where dy1 > 0 and dy2 == 0 for runs -> keep first index
    flats = np.where((dy1 > 0) & (dy2 == 0))[0] + 1
    if flats.size:
        candidates = np.unique(np.concatenate([candidates, flats]))
    return candidates.astype(int)

def _enforce_min_distance(peaks: np.ndarray, y: np.ndarray, min_dist: int) -> np.ndarray:
    if peaks.size == 0:
        return peaks
    # Greedy keep the tallest within windows
    order = np.argsort(-y[peaks])  # descending by height
    picked = []
    taken = np.zeros(y.size, dtype=bool)
    for idx in order:
        p = int(peaks[idx])
        if taken[p]:
            continue
        picked.append(p)
        left = max(0, p - min_dist)
        right = min(y.size, p + min_dist + 1)
        taken[left:right] = True
    picked.sort()
    return np.asarray(picked, dtype=int)

def _choose_peak_polarity(y: np.ndarray, min_dist: int) -> np.ndarray:
    # Try maxima and minima; pick the set with more regular spacing (lower gap MAD)
    peaks_max = _enforce_min_distance(_find_local_maxima(y), y, min_dist)
    peaks_min = _enforce_min_distance(_find_local_maxima(-y), -y, min_dist)
    def score(peaks):
        if peaks.size < 4:
            return np.inf
        gaps = np.diff(peaks)
        med = np.median(gaps)
        mad = 1.4826 * np.median(np.abs(gaps - med))
        return mad
    s_max = score(peaks_max)
    s_min = score(peaks_min)
    if s_max == np.inf and s_min == np.inf:
        return np.empty(0, dtype=int)
    return peaks_max if s_max <= s_min else peaks_min

# --------------------------- Template construction -------------------------- #

def _cycle_segments(tu: np.ndarray, yu: np.ndarray, peaks: np.ndarray) -> List[Tuple[int, int]]:
    # Boundaries at midpoints between successive peaks
    if peaks.size < 3:
        return []
    mids = ((peaks[:-1] + peaks[1:]) // 2).astype(int)
    starts = np.concatenate(([max(0, peaks[0] - (mids[0] - peaks[0]))], mids))
    ends = np.concatenate((mids, [min(yu.size, peaks[-1] + (peaks[-1] - mids[-1]))]))
    segs = []
    for s, e in zip(starts, ends):
        s = int(max(0, s))
        e = int(min(yu.size, e))
        if e - s >= 8:
            segs.append((s, e))
    return segs

def _resample_unit(seg: np.ndarray, L: int) -> np.ndarray:
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    z = seg - np.mean(seg)
    s = float(np.std(seg))
    if s > 0:
        z = z / s
    return np.interp(np.linspace(0, 1, L, endpoint=False), xi, z)

def _phase_invariant_corr(a: np.ndarray, b: np.ndarray) -> float:
    # a, b are unit-length vectors in same length space (not necessarily unit-norm yet)
    if a.size != b.size:
        L = max(a.size, b.size)
        a = np.interp(np.linspace(0, 1, L, endpoint=False), np.linspace(0, 1, a.size, endpoint=False), a)
        b = np.interp(np.linspace(0, 1, L, endpoint=False), np.linspace(0, 1, b.size, endpoint=False), b)
    # unit norm
    na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))
    if na == 0 or nb == 0:
        return 0.0
    a = a / na; b = b / nb
    c = np.fft.ifft(np.fft.fft(a) * np.conj(np.fft.fft(b))).real
    c = np.clip(c, -1.0, 1.0)
    return float(np.max(c))

def _pick_stable_training_cycles(cycles: List[np.ndarray], max_train: int = 12, L: int = 128) -> List[int]:
    # Build z-normalized, resampled cycles
    Z = [ _resample_unit(c, L) for c in cycles[:max(20, max_train)] ]  # consider up to first 20 cycles
    if len(Z) < 4:
        return list(range(min(len(Z), max_train)))
    M = len(Z)
    # Correlation matrix
    C = np.zeros((M, M), dtype=float)
    for i in range(M):
        C[i, i] = 1.0
        for j in range(i+1, M):
            cij = _phase_invariant_corr(Z[i], Z[j])
            C[i, j] = C[j, i] = cij
    # Choose center cycle with highest mean corr
    mean_corr = np.mean(C, axis=1)
    center = int(np.argmax(mean_corr))
    # Rank other cycles by correlation to center
    order = np.argsort(-C[center])
    # Keep top cycles with corr close to center's distribution
    selected = []
    base = C[center, order]
    med, sig = _robust_mean_std(base)
    thr = max(0.85, med - 2.0 * sig)  # require fairly high similarity
    for idx in order:
        if C[center, idx] >= thr:
            selected.append(idx)
        if len(selected) >= max_train:
            break
    selected.sort()
    return selected

def _build_template_from_cycles(cycles: List[np.ndarray], idxs: List[int], L: int = 128) -> np.ndarray:
    if not idxs:
        return np.array([], dtype=float)
    acc = np.zeros(L, dtype=float)
    cnt = 0
    for k in idxs:
        z = _resample_unit(cycles[k], L)
        acc += z; cnt += 1
    tpl = acc / max(1, cnt)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    return tpl

# ---------------------------- Local refine inside cycle --------------------- #

def _refine_within_cycle_time(tu: np.ndarray, yu: np.ndarray, seg_bounds: Tuple[int, int], template: np.ndarray) -> float:
    s, e = seg_bounds
    seg = yu[s:e]
    L = template.size
    # Align the cycle to template (phase-invariant)
    z = _resample_unit(seg, L)
    # Find best circular shift to align
    c = np.fft.ifft(np.fft.fft(z) * np.conj(np.fft.fft(template))).real
    shift = int(np.argmax(c)) % L
    # Apply shift to the original (unresampled) segment by mapping indices
    # We'll operate in the resampled domain to locate the change, then map to time.
    # Sliding window correlation against template, using short window (~1/6 period)
    w = max(8, L // 6)
    hold = max(3, w // 6)  # require a few consecutive dips
    # Precompute template windows norms
    tpl = np.roll(template, -shift)  # template aligned to z
    # rolling windows
    bad = []
    for i in range(0, L - w + 1):
        win = z[i:i+w]
        twn = tpl[i:i+w]
        n1 = float(np.linalg.norm(win)); n2 = float(np.linalg.norm(twn))
        corr = float(np.dot(win, twn) / (n1 * n2)) if (n1 > 0 and n2 > 0) else 0.0
        bad.append(corr < 0.55)
    bad = np.asarray(bad, dtype=bool)
    # first sustained run
    run = 0
    pos = None
    for i, b in enumerate(bad):
        if b:
            run += 1
            if run >= hold:
                pos = i - hold + 1
                break
        else:
            run = 0
    if pos is None:
        # fallback: use cycle start
        return float(tu[s])
    # Map resampled index to original time
    alpha = (pos / max(1, L - 1))
    t0 = float(tu[s])
    t1 = float(tu[e-1])
    return float(t0 + alpha * (t1 - t0))

# --------------------------------- Main ------------------------------------- #

def pattern_major_fault_time(signal_view) -> float:
    x, y = _as_numpy_xy(signal_view)
    if x.size < 256:
        return float("nan")

    # 1) Uniform resample
    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 256:
        return float("nan")

    # 2) Period estimate on early region
    learn_fraction = 0.30
    N = tu.size
    N_learn = min(max(256, int(N * learn_fraction)), max(256, N // 2))
    yL = yu[:N_learn]
    T = _best_period(yL, dt)
    if not np.isfinite(T):
        return float("nan")
    spp = int(round(T / dt))
    if spp < 12:
        return float("nan")

    # 3) Peaks and cycles
    min_dist = max(4, int(round(0.4 * spp)))
    peaks = _choose_peak_polarity(yu, min_dist)
    if peaks.size < 6:
        return float("nan")
    segs = _cycle_segments(tu, yu, peaks)
    if len(segs) < 6:
        return float("nan")

    # 4) Build cycles array and choose stable training set
    cycles = [ yu[s:e] for (s, e) in segs ]
    train_idxs = _pick_stable_training_cycles(cycles, max_train=12, L=128)
    if not train_idxs:
        return float("nan")
    # Ensure training cycles end before or near learn window
    # If many cycles start after learn window, trim to earliest ones
    cutoff_time = tu[min(N_learn - 1, N - 1)]
    trimmed = [k for k in train_idxs if segs[k][1] <= np.searchsorted(tu, cutoff_time, side="right")]
    if len(trimmed) >= max(4, len(train_idxs)//2):
        train_idxs = trimmed

    template = _build_template_from_cycles(cycles, train_idxs, L=128)
    if template.size == 0:
        return float("nan")

    # 5) Score cycles by shape-only, phase-invariant correlation
    Ltpl = template.size
    cycle_corr = []
    cycle_time = []
    for (s, e) in segs:
        z = _resample_unit(yu[s:e], Ltpl)
        c = _phase_invariant_corr(z, template)
        cycle_corr.append(c)
        cycle_time.append(tu[s])  # start time of cycle
    cycle_corr = np.asarray(cycle_corr, dtype=float)
    cycle_time = np.asarray(cycle_time, dtype=float)

    # 6) Baseline stats from training cycles
    base_corr = cycle_corr[train_idxs]
    mc, sc = _robust_mean_std(base_corr)
    if not np.isfinite(mc) or not np.isfinite(sc):
        return float("nan")
    # Threshold: strong drop and low absolute corr
    corr_thr = min(mc - 5.0 * sc, 0.45)

    # 7) Find first anomalous cycle after the training region
    last_train = max(train_idxs)
    start_idx = last_train + 1
    if start_idx >= len(segs):
        return float("nan")

    # Smooth cycle correlations a bit to avoid minute blips
    corr_s = _median_filter(cycle_corr, k=3)
    bad = corr_s < corr_thr

    # Require at least 2 consecutive bad cycles to be robust
    run = 0
    j_star = None
    for j in range(start_idx, len(segs)):
        if bad[j]:
            run += 1
            if run >= 2:
                j_star = j - 1  # mark from the first bad in the run
                break
        else:
            run = 0
    if j_star is None:
        return float("nan")

    # 8) Refine timestamp within the first anomalous cycle
    t_refined = _refine_within_cycle_time(tu, yu, segs[j_star], template)
    return float(t_refined)

__all__ = ["pattern_major_fault_time"]
