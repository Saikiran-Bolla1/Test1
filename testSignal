"""
Detect the timestamp of a MAJOR disturbance in a repeating signal pattern (shape-only)
and avoid early triggers before the actual fault time.

What changed to stop early detections (e.g., 4.70 before a real 4.81):
- Shape-only, phase-invariant correlation (offset/scale removed, circular alignment).
- Pre-change stability gate: must see a block of high correlation immediately before fault.
- Long persistence + confirmation: low correlation must persist and its post-mean must be low.
- Derivative gate: require a sharp negative drop at the onset, not slow drift.
- Offline change-point fallback on the correlation series and return the threshold crossing
  at that change, not the first small dip.

Public API
- pattern_major_fault_time(signal_view) -> float
  signal_view: object with .x (timestamps) and .y (values), or a tuple (x, y)
  Returns first timestamp (seconds) of a major pattern disturbance, or float("nan").
"""

from typing import Tuple, List
import numpy as np

# ------------------------------ Utilities ---------------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)
    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]
    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _median_filter(v: np.ndarray, k: int = 5) -> np.ndarray:
    if k <= 1 or v.size == 0:
        return v.copy()
    if k % 2 == 0:
        k += 1
    r = k // 2
    pad = np.pad(v, (r, r), mode="reflect")
    out = np.empty_like(v)
    for i in range(v.size):
        out[i] = np.median(pad[i:i+k])
    return out

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

# ---------------------------- Period estimation ----------------------------- #

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0
    fmin = 3.0 / (N * dt)
    fmax = 0.45 / dt
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    k = int(np.argmax(mag[band]))
    idx = np.flatnonzero(band)[k]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

# --------------------------- Shape-only template ---------------------------- #

def _resample_unit(seg: np.ndarray, L: int) -> np.ndarray:
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    z = seg - np.mean(seg)
    s = float(np.std(seg))
    if s > 0:
        z = z / s
    else:
        z = z * 0.0
    return np.interp(np.linspace(0, 1, L, endpoint=False), xi, z)

def _phase_invariant_corr(a: np.ndarray, b: np.ndarray) -> float:
    na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))
    if na == 0.0 or nb == 0.0:
        return 0.0
    a = a / na; b = b / nb
    c = np.fft.ifft(np.fft.fft(a) * np.conj(np.fft.fft(b))).real
    return float(np.clip(np.max(c), -1.0, 1.0))

def _template_from_learning(yu: np.ndarray, spp: int, L: int, learn_samples: int) -> np.ndarray:
    hop = max(1, spp // 8)
    windows = []
    for i in range(0, learn_samples - spp, hop):
        windows.append(_resample_unit(yu[i:i+spp], L))
    if len(windows) < 6:
        return np.array([], dtype=float)
    W = np.stack(windows, axis=0)
    tpl = np.mean(W, axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    scores = np.array([_phase_invariant_corr(w, tpl) for w in W], dtype=float)
    order = np.argsort(-scores)
    K = min(16, max(8, len(order)//2))
    top = W[order[:K]]
    tpl = np.mean(top, axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    return tpl

# ------------------------------ Correlation TS ------------------------------ #

def _correlation_series(tu: np.ndarray, yu: np.ndarray, spp: int, L: int, template: np.ndarray) -> Tuple[np.ndarray, np.ndarray, int]:
    hop = max(1, spp // 8)  # ~8 hops/period
    times, corr = [], []
    N = yu.size
    for i in range(0, N - spp, hop):
        seg = yu[i:i+spp]
        z = _resample_unit(seg, L)
        c = _phase_invariant_corr(z, template)
        times.append(tu[i + spp // 2])
        corr.append(c)
    return np.asarray(times, dtype=float), np.asarray(corr, dtype=float), hop

# ------------------------- Change-point fallback ---------------------------- #

def _argmax_change_point(series: np.ndarray, start_idx: int, min_left: int, min_right: int) -> int:
    """
    Offline single change-point by maximizing within-segment SSE reduction.
    Returns index j (split between j and j+1). -1 if not found.
    """
    n = series.size
    if n < start_idx + min_left + min_right + 2:
        return -1
    x = series
    c1 = np.cumsum(x)
    c2 = np.cumsum(x*x)
    best_j, best_gain = -1, 0.0
    for j in range(start_idx + min_left, n - min_right - 1):
        n1 = j + 1
        n2 = n - n1
        s1 = c1[j] - (c1[start_idx-1] if start_idx > 0 else 0.0)
        s2 = (c1[-1] - c1[j])
        ss1 = c2[j] - (c2[start_idx-1] if start_idx > 0 else 0.0)
        ss2 = c2[-1] - c2[j]
        n1_eff = n1 - start_idx
        n2_eff = n2
        if n1_eff <= 0 or n2_eff <= 0:
            continue
        m1 = s1 / n1_eff
        m2 = s2 / n2_eff
        sse1 = ss1 - n1_eff * m1 * m1
        sse2 = ss2 - n2_eff * m2 * m2
        total_sse = sse1 + sse2
        # Baseline SSE (no split) over [start_idx, n-1]
        s_all = c1[-1] - (c1[start_idx-1] if start_idx > 0 else 0.0)
        ss_all = c2[-1] - (c2[start_idx-1] if start_idx > 0 else 0.0)
        n_all = n - start_idx
        m_all = s_all / n_all
        sse_all = ss_all - n_all * m_all * m_all
        gain = sse_all - total_sse
        if gain > best_gain:
            best_gain, best_j = gain, j
    return best_j

# --------------------- Strict first-drop with confirmation ------------------ #

def _first_confirmed_drop_time(times: np.ndarray,
                               corr_s: np.ndarray,
                               learn_end_time: float,
                               mc: float, sc: float,
                               spp: int, hop: int) -> float:
    low_thr = min(mc - 5.0 * sc, 0.40)
    pre_high_thr = max(0.78, mc - 0.8 * sc)
    hold_low = max(4, int(round(1.2 * spp / hop)))        # ~1.2 periods low
    confirm_win = max(4, int(round(0.8 * spp / hop)))     # confirm next ~0.8 period stays low
    pre_need = max(3, int(round(0.8 * spp / hop)))        # need ~0.8 period high before

    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    if j0 >= times.size:
        return float("nan")

    # Derivative gate (use robust baseline of diffs)
    diffs = np.diff(corr_s[:max(j0, 3)]) if j0 > 3 else np.array([0.0])
    _, sd_diff = _robust_mean_std(diffs)
    drop_deriv_thr = max(3.0 * sd_diff, 0.06)

    below = corr_s < low_thr
    run = 0
    for j in range(j0, times.size):
        if below[j]:
            run += 1
        else:
            run = 0
        if run >= hold_low:
            s = j - run + 1  # start of low run
            # Pre-high gate
            p0 = max(j0, s - pre_need)
            if not (p0 < s and np.all(corr_s[p0:s] > pre_high_thr)):
                continue
            # Deriv gate at onset (use last high -> first low step)
            k = s - 1
            while k >= j0 and below[k]:
                k -= 1
            if k < j0:
                k = s
            if k > 0:
                step = corr_s[k+1] - corr_s[k]
                if step > -drop_deriv_thr:
                    continue

            # Confirmation: the next block should stay low on average
            tail = corr_s[s: s + confirm_win]
            if tail.size == 0 or float(np.mean(tail)) > (low_thr - 0.03):
                continue

            # Interpolate crossing between last high and first low
            if k >= j0 and k + 1 < times.size:
                y0, y1 = float(corr_s[k]), float(corr_s[k+1])
                t0, t1 = float(times[k]), float(times[k+1])
                if t1 != t0 and y1 != y0:
                    alpha = (low_thr - y0) / (y1 - y0)
                    alpha = float(np.clip(alpha, 0.0, 1.0))
                    return float(t0 + alpha * (t1 - t0))
            return float(times[s])
    return float("nan")

# --------------------------------- Main ------------------------------------- #

def pattern_major_fault_time(signal_view) -> float:
    x, y = _as_numpy_xy(signal_view)
    if x.size < 128:
        return float("nan")

    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 128:
        return float("nan")

    # Learn window (~30% capped at half)
    N = tu.size
    N_learn = min(max(128, int(0.30 * N)), max(128, N // 2))
    learn_end_time = float(tu[min(N_learn - 1, N - 1)])
    yL = yu[:N_learn]

    # Estimate period and samples per period
    T = _best_period(yL, dt)
    if not np.isfinite(T):
        return float("nan")
    spp = int(round(T / dt))
    if spp < 12 or spp > N_learn // 2:
        return float("nan")

    # Template from learning region
    L = 128
    template = _template_from_learning(yu, spp, L, learn_samples=N_learn)
    if template.size == 0:
        return float("nan")

    # Correlation time series across full record
    times, corr, hop = _correlation_series(tu, yu, spp, L, template)
    if times.size == 0:
        return float("nan")

    # Smooth slightly to suppress minute blips
    corr_s = _median_filter(corr, k=5)

    # Baseline stats inside learn region
    learn_mask = times <= learn_end_time
    if not np.any(learn_mask):
        return float("nan")
    mc, sc = _robust_mean_std(corr_s[learn_mask])
    if not np.isfinite(mc) or not np.isfinite(sc):
        return float("nan")

    # Primary: strict first-drop with stability + confirmation + derivative gate
    t_primary = _first_confirmed_drop_time(times, corr_s, learn_end_time, mc, sc, spp, hop)

    # Fallback: offline change-point on correlation and take threshold-crossing near CP
    t_cp = float("nan")
    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    min_left = max(4, int(round(0.8 * spp / hop)))
    min_right = max(4, int(round(0.8 * spp / hop)))
    jcp = _argmax_change_point(corr_s, j0, min_left, min_right)
    if jcp != -1:
        # Validate CP: post mean significantly lower than pre mean
        pre = corr_s[max(j0, jcp - min_left): jcp + 1]
        post = corr_s[jcp + 1: min(times.size, jcp + 1 + min_right)]
        if pre.size > 0 and post.size > 0:
            pre_m = float(np.mean(pre))
            post_m = float(np.mean(post))
            drop = pre_m - post_m
            if drop >= max(0.20, 3.0 * sc):
                low_thr = min(mc - 5.0 * sc, 0.40)
                # Walk backward from CP to find first crossing of low_thr
                k = jcp
                while k > j0 and corr_s[k-1] >= low_thr:
                    k -= 1
                # Interpolate between k-1 and k
                if k > j0:
                    y0, y1 = float(corr_s[k-1]), float(corr_s[k])
                    t0, t1 = float(times[k-1]), float(times[k])
                    if t1 != t0 and y1 != y0:
                        alpha = (low_thr - y0) / (y1 - y0)
                        alpha = float(np.clip(alpha, 0.0, 1.0))
                        t_cp = float(t0 + alpha * (t1 - t0))
                    else:
                        t_cp = float(times[k])
                else:
                    t_cp = float(times[max(j0, k)])

    # Choose the later of the two (to avoid early false starts), but if only one exists use it
    candidates = [t for t in (t_primary, t_cp) if np.isfinite(t)]
    if not candidates:
        return float("nan")
    if len(candidates) == 2:
        return float(max(candidates))
    return float(candidates[0])

__all__ = ["pattern_major_fault_time"]
