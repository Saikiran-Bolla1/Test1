from typing import Tuple
import numpy as np

# ------------------------------ Utilities ---------------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    # Accept (x, y) tuple or object with .x/.y
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)

    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")

    # Drop NaNs (synchronized)
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]

    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]

    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    """Resample to a uniform grid using linear interpolation. Returns (tu, yu, dt)."""
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    """Median and robust sigma via MAD (scaled). Falls back to std if needed."""
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

# ---------------------------- Period estimation ----------------------------- #

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via FFT of zero-mean data; NaN if not reliable."""
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0  # ignore DC
    # at least ~3 cycles in window and <= 90% Nyquist
    fmin = 3.0 / (N * dt)
    fmax = 0.45 / dt
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    k = np.argmax(mag[band])
    idx = np.flatnonzero(band)[k]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via autocorrelation peak; NaN if not reliable."""
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

# --------------------------- Template construction -------------------------- #

def _align_and_average(y: np.ndarray, spp: int, max_periods: int = 12, template_points: int = 128) -> np.ndarray:
    """
    Build a normalized template by aligning up to max_periods cycles (length spp),
    using circular shift to maximize correlation with the first cycle.
    """
    n = y.size
    spp = int(max(8, spp))
    K = min(max_periods, n // spp)
    if K < 2:
        return np.array([], dtype=float)

    ref = y[0:spp].astype(float)
    ref = ref - np.mean(ref)
    s = float(np.std(ref))
    if s > 0:
        ref /= s

    acc = np.zeros(spp, dtype=float)
    cnt = 0
    for i in range(K):
        seg = y[i * spp:(i + 1) * spp].astype(float)
        if seg.size != spp:
            break
        seg = seg - np.mean(seg)
        std = float(np.std(seg))
        if std > 0:
            seg = seg / std
        # align by circular shift maximizing correlation
        c = np.fft.ifft(np.fft.fft(ref) * np.conj(np.fft.fft(seg))).real
        shift = int(np.argmax(c)) % spp
        seg_aligned = np.roll(seg, shift)
        acc += seg_aligned
        cnt += 1

    if cnt == 0:
        return np.array([], dtype=float)

    avg = acc / cnt
    # resample to a fixed phase grid for stable comparison
    xi = np.linspace(0, 1, spp, endpoint=False)
    phase_grid = np.linspace(0, 1, template_points, endpoint=False)
    tpl = np.interp(phase_grid, xi, avg)
    # unit norm
    norm = float(np.linalg.norm(tpl))
    if norm > 0:
        tpl = tpl / norm
    return tpl

def _window_features(y: np.ndarray, i: int, spp: int, template: np.ndarray) -> Tuple[float, float, float, float]:
    """
    Features for window y[i:i+spp]:
    - correlation to template (normalized)
    - mean
    - std
    - peak-to-peak amplitude
    """
    seg = y[i:i + spp]
    if seg.size != spp:
        return np.nan, np.nan, np.nan, np.nan
    m = float(np.mean(seg))
    s = float(np.std(seg))
    a = float(np.max(seg) - np.min(seg))
    # normalized correlation
    z = seg - m
    if s > 0:
        z = z / s
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    seg_t = np.interp(np.linspace(0, 1, template.size, endpoint=False), xi, z)
    denom = float(np.linalg.norm(seg_t))
    c = float(np.dot(seg_t, template) / denom) if denom > 0 else 0.0
    return c, m, s, a

# ------------------------------- CUSUM -------------------------------------- #

def _cusum_first_time(series: np.ndarray, times: np.ndarray, baseline: float, sigma: float, k_sigma: float, h_sigma: float, start_idx: int) -> float:
    """
    One-sided two-direction CUSUM on a windowed feature series.
    Returns first time where either pos or neg CUSUM crosses h_sigma*sigma, else NaN.
    """
    if not np.isfinite(sigma) or sigma == 0:
        return float("nan")
    k = k_sigma * sigma
    h = h_sigma * sigma

    s_pos = 0.0
    s_neg = 0.0
    for j in range(start_idx, series.size):
        x = float(series[j]) - baseline
        s_pos = max(0.0, s_pos + (x - k))
        s_neg = min(0.0, s_neg + (x + k))
        if s_pos > h or abs(s_neg) > h:
            return float(times[j])
    return float("nan")

# ------------------------------ Main detector ------------------------------- #

def pattern_change_time_robust(signal_view) -> float:
    """
    Return the first timestamp (float seconds) where a MAJOR pattern change occurs.
    Minor variations are ignored by design. Returns float('nan') if none detected.

    Latency minimizations vs previous version:
    - Window timestamps are at the START of the window (not center).
    - Finer hop (~T/12) to reduce time quantization.
    - Linear interpolation at the first correlation threshold crossing.
    """
    x, y = _as_numpy_xy(signal_view)
    if x.size < 256:
        return float("nan")

    # Uniform resample
    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 256:
        return float("nan")

    # Learn baseline window
    learn_fraction = 0.30
    N = tu.size
    N_learn = min(max(256, int(N * learn_fraction)), max(256, N // 2))
    yL = yu[:N_learn]

    # Period estimate
    T = _best_period(yL, dt)
    if not np.isfinite(T):
        return float("nan")
    spp = int(round(T / dt))
    if spp < 12 or spp > N_learn // 2:
        return float("nan")

    # Template from learning region
    template = _align_and_average(yL, spp, max_periods=12, template_points=128)
    if template.size == 0:
        return float("nan")

    # Sliding features across full record
    # Finer hop for earlier detection while keeping compute reasonable
    hop = max(1, int(round(spp / 12)))  # ~12 hops per period (was ~8)
    times = []
    corr = []
    meanv = []
    stdv = []
    ampv = []
    for i in range(0, N - spp, hop):
        c, m, s, a = _window_features(yu, i, spp, template)
        # Timestamp at START of window to avoid ~T/2 latency
        times.append(tu[i])
        corr.append(c)
        meanv.append(m)
        stdv.append(s)
        ampv.append(a)
    times = np.asarray(times, dtype=float)
    corr = np.asarray(corr, dtype=float)
    meanv = np.asarray(meanv, dtype=float)
    stdv = np.asarray(stdv, dtype=float)
    ampv = np.asarray(ampv, dtype=float)
    if times.size == 0:
        return float("nan")

    # Baseline stats (learn region)
    learn_end_time = tu[min(N_learn - 1, N - 1)]
    learn_mask = times <= learn_end_time
    if not np.any(learn_mask):
        return float("nan")

    mc, sc = _robust_mean_std(corr[learn_mask])
    mm, sm = _robust_mean_std(meanv[learn_mask])
    ms, ss = _robust_mean_std(stdv[learn_mask])
    ma, sa = _robust_mean_std(ampv[learn_mask])
    if not all(np.isfinite(v) for v in (mc, sc, mm, sm, ms, ss, ma, sa)):
        return float("nan")

    # Detector A: correlation drop (shape change), sustained
    # Strict threshold to ignore minute changes
    corr_thr = min(mc - 4.5 * sc, 0.45)
    # Slightly shorter sustain to reduce latency while keeping robustness
    hold_windows = max(3, int(round(0.3 * spp / hop)))  # ~30% of a period sustained

    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    bad_corr = corr < corr_thr

    tA = float("nan")
    run = 0
    jA_start = None
    for j in range(j0, times.size):
        if bad_corr[j]:
            run += 1
            if run >= hold_windows:
                jA_start = j - hold_windows + 1
                # Linear interpolate the first threshold crossing between the last good and first bad
                k = jA_start - 1
                if k >= j0 and np.isfinite(corr[k]):
                    y1, t1 = corr[k], times[k]
                    y2, t2 = corr[jA_start], times[jA_start]
                    if y2 != y1:
                        alpha = (corr_thr - y1) / (y2 - y1)
                        alpha = float(np.clip(alpha, 0.0, 1.0))
                        tA = t1 + alpha * (t2 - t1)
                    else:
                        tA = times[jA_start]
                else:
                    tA = times[jA_start]
                break
        else:
            run = 0

    # Detector B: CUSUM on window mean (level shift)
    # Keeping conservative thresholds; reduce h_sigma slightly for earlier trigger.
    tB = _cusum_first_time(meanv, times, mm, sm, k_sigma=0.5, h_sigma=7.0, start_idx=j0)

    # Detector C: CUSUM on window amplitude (pulse height/width changes)
    tC = _cusum_first_time(ampv, times, ma, sa, k_sigma=0.5, h_sigma=7.0, start_idx=j0)

    # Choose earliest valid time
    candidates = [t for t in (tA, tB, tC) if np.isfinite(t)]
    if not candidates:
        return float("nan")
    return float(min(candidates))

__all__ = ["pattern_change_time_robust"]
