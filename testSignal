"""
Detect the timestamp of a MAJOR disturbance in a repeating signal pattern (shape-only) with robust fallbacks.

Why this should fix your NaN:
- If period estimation or cycle segmentation fails, it falls back to a windowed,
  template-free shape detector (no explicit period needed).
- Ignores minute offset and amplitude changes by z-normalizing windows.
- Uses phase-invariant correlation to handle small timing jitter.
- Returns sub-sample time via interpolation when possible.
- Still triggers only on significant, sustained changes (not tiny blips).

Public API (drop-in):
- pattern_major_fault_time(signal_view) -> float
  signal_view: object with .x (timestamps) and .y (values), or a tuple (x, y)
  returns: first timestamp (seconds) of a major pattern disturbance, or float("nan")

Internal policy (no user tuning):
- Learn baseline from first ~30% (capped at half).
- Primary: cycle-based, shape-only correlation vs learned template (>=5σ drop and <0.45),
  sustained for ~50% of a period.
- Fallback A (no period required): windowed shape-only correlation vs learned window-template,
  sustained for ~30% of window length.
- Fallback B: if still nothing, pick the strongest post-baseline correlation drop if
  it is both large in absolute terms (>=0.25) and relative (>=3σ).
"""

from typing import Tuple, List
import numpy as np

# ------------------------------ Utilities ---------------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)
    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")
    # Drop NaNs, synchronized
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]
    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]
    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

def _median_filter(v: np.ndarray, k: int = 5) -> np.ndarray:
    if k <= 1 or v.size == 0:
        return v.copy()
    if k % 2 == 0:
        k += 1
    r = k // 2
    pad = np.pad(v, (r, r), mode="reflect")
    out = np.empty_like(v)
    for i in range(v.size):
        out[i] = np.median(pad[i:i+k])
    return out

def _first_sustained_crossing_time(times: np.ndarray, series: np.ndarray, threshold: float, hold_windows: int, start_idx: int) -> float:
    """
    First time where series stays below threshold for hold_windows consecutive steps.
    Interpolates the crossing at the start of the sustained run.
    """
    below = series < threshold
    run = 0
    for j in range(start_idx, series.size):
        if below[j]:
            run += 1
            if run >= hold_windows:
                s = j - hold_windows + 1
                k = s - 1
                while k >= 0 and below[k]:
                    k -= 1
                if k < 0:
                    return float(times[s])
                y0, y1 = float(series[k]), float(series[k + 1])
                t0, t1 = float(times[k]), float(times[k + 1])
                if y1 == y0 or t1 == t0:
                    return float(times[s])
                alpha = (threshold - y0) / (y1 - y0)
                alpha = float(np.clip(alpha, 0.0, 1.0))
                return float(t0 + alpha * (t1 - t0))
        else:
            run = 0
    return float("nan")

# ---------------------------- Period estimation ----------------------------- #

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0
    fmin = 3.0 / (N * dt)
    fmax = 0.45 / dt
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    k = int(np.argmax(mag[band]))
    idx = np.flatnonzero(band)[k]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

# ----------------------------- Peak detection ------------------------------- #

def _find_local_maxima(y: np.ndarray) -> np.ndarray:
    n = y.size
    if n < 3:
        return np.empty(0, dtype=int)
    dy1 = y[1:-1] - y[:-2]
    dy2 = y[2:] - y[1:-1]
    candidates = np.where((dy1 > 0) & (dy2 <= 0))[0] + 1
    flats = np.where((dy1 > 0) & (dy2 == 0))[0] + 1
    if flats.size:
        candidates = np.unique(np.concatenate([candidates, flats]))
    return candidates.astype(int)

def _enforce_min_distance(peaks: np.ndarray, y: np.ndarray, min_dist: int) -> np.ndarray:
    if peaks.size == 0:
        return peaks
    order = np.argsort(-y[peaks])  # tallest first
    picked = []
    taken = np.zeros(y.size, dtype=bool)
    for idx in order:
        p = int(peaks[idx])
        if taken[p]:
            continue
        picked.append(p)
        left = max(0, p - min_dist)
        right = min(y.size, p + min_dist + 1)
        taken[left:right] = True
    picked.sort()
    return np.asarray(picked, dtype=int)

def _choose_peak_polarity(y: np.ndarray, min_dist: int) -> np.ndarray:
    peaks_max = _enforce_min_distance(_find_local_maxima(y), y, min_dist)
    peaks_min = _enforce_min_distance(_find_local_maxima(-y), -y, min_dist)
    def score(peaks):
        if peaks.size < 4:
            return np.inf
        gaps = np.diff(peaks)
        med = np.median(gaps)
        mad = 1.4826 * np.median(np.abs(gaps - med))
        return mad
    s_max = score(peaks_max)
    s_min = score(peaks_min)
    if s_max == np.inf and s_min == np.inf:
        return np.empty(0, dtype=int)
    return peaks_max if s_max <= s_min else peaks_min

# --------------------------- Template construction -------------------------- #

def _cycle_segments(yu: np.ndarray, peaks: np.ndarray) -> List[Tuple[int, int]]:
    if peaks.size < 3:
        return []
    mids = ((peaks[:-1] + peaks[1:]) // 2).astype(int)
    starts = np.concatenate(([max(0, peaks[0] - (mids[0] - peaks[0]))], mids))
    ends = np.concatenate((mids, [min(yu.size, peaks[-1] + (peaks[-1] - mids[-1]))]))
    segs = []
    for s, e in zip(starts, ends):
        s = int(max(0, s))
        e = int(min(yu.size, e))
        if e - s >= 8:
            segs.append((s, e))
    return segs

def _resample_unit(seg: np.ndarray, L: int) -> np.ndarray:
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    z = seg - np.mean(seg)
    s = float(np.std(seg))
    if s > 0:
        z = z / s
    return np.interp(np.linspace(0, 1, L, endpoint=False), xi, z)

def _phase_invariant_corr(a: np.ndarray, b: np.ndarray) -> float:
    if a.size != b.size:
        L = max(a.size, b.size)
        a = np.interp(np.linspace(0, 1, L, endpoint=False), np.linspace(0, 1, a.size, endpoint=False), a)
        b = np.interp(np.linspace(0, 1, L, endpoint=False), np.linspace(0, 1, b.size, endpoint=False), b)
    na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))
    if na == 0 or nb == 0:
        return 0.0
    a = a / na; b = b / nb
    c = np.fft.ifft(np.fft.fft(a) * np.conj(np.fft.fft(b))).real
    c = np.clip(c, -1.0, 1.0)
    return float(np.max(c))

def _build_template_from_cycles(cycles: List[np.ndarray], max_train: int = 12, L: int = 128) -> Tuple[np.ndarray, List[int]]:
    if len(cycles) == 0:
        return np.array([], dtype=float), []
    Z = [ _resample_unit(c, L) for c in cycles[:max(24, max_train)] ]
    M = len(Z)
    if M < 4:
        # average what we have
        acc = np.sum(Z, axis=0)
        tpl = acc / max(1, len(Z))
        n = float(np.linalg.norm(tpl))
        if n > 0:
            tpl = tpl / n
        return tpl, list(range(len(Z)))
    # Correlation matrix
    C = np.eye(M, dtype=float)
    for i in range(M):
        for j in range(i+1, M):
            cij = _phase_invariant_corr(Z[i], Z[j])
            C[i, j] = C[j, i] = cij
    mean_corr = np.mean(C, axis=1)
    center = int(np.argmax(mean_corr))
    # Rank by similarity to center
    order = np.argsort(-C[center])
    base = C[center, order]
    med, sig = _robust_mean_std(base)
    thr = max(0.85, med - 2.0 * sig)
    selected = [idx for idx in order if C[center, idx] >= thr][:max_train]
    selected.sort()
    tpl = np.mean([Z[k] for k in selected], axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    return tpl, selected

# ---------------------------- Local refine inside cycle --------------------- #

def _refine_within_cycle_time(tu: np.ndarray, yu: np.ndarray, seg_bounds: Tuple[int, int], template: np.ndarray) -> float:
    s, e = seg_bounds
    seg = yu[s:e]
    L = template.size
    z = _resample_unit(seg, L)
    # Best circular shift to align
    c = np.fft.ifft(np.fft.fft(z) * np.conj(np.fft.fft(template))).real
    shift = int(np.argmax(c)) % L
    tpl = np.roll(template, -shift)
    # Short window scan ~1/6 period
    w = max(8, L // 6)
    hold = max(3, w // 6)
    bad = []
    for i in range(0, L - w + 1):
        win = z[i:i+w]
        twn = tpl[i:i+w]
        n1 = float(np.linalg.norm(win)); n2 = float(np.linalg.norm(twn))
        corr = float(np.dot(win, twn) / (n1 * n2)) if (n1 > 0 and n2 > 0) else 0.0
        bad.append(corr < 0.55)
    bad = np.asarray(bad, dtype=bool)
    run = 0
    pos = None
    for i, b in enumerate(bad):
        if b:
            run += 1
            if run >= hold:
                pos = i - hold + 1
                break
        else:
            run = 0
    if pos is None:
        return float(tu[s])
    alpha = (pos / max(1, L - 1))
    t0 = float(tu[s]); t1 = float(tu[e-1])
    return float(t0 + alpha * (t1 - t0))

# ---------------------------- Windowed fallback (no T) ---------------------- #

def _window_shape_change_time(tu: np.ndarray, yu: np.ndarray, learn_end_time: float) -> float:
    """
    Fallback: detect shape change without knowing the period.
    - Build a shape-only template from baseline windows (first ~30% of record).
    - Slide windows over the full signal and compute phase-invariant correlation.
    - Trigger on a large, sustained drop.
    """
    N = tu.size
    if N < 128:
        return float("nan")

    # Choose a window ~ N/20 (5% of the record), clipped
    W = int(np.clip(N // 20, 64, N // 6))
    hop = max(1, W // 4)
    L = 128  # template length

    # Build baseline windows ending before learn_end_time
    times = []
    corr = []
    windows = []
    win_times = []
    for i in range(0, N - W, hop):
        seg = yu[i:i+W]
        z = _resample_unit(seg, L)
        windows.append(z)
        win_times.append(tu[i + W // 2])
    windows = np.asarray(windows, dtype=float)
    win_times = np.asarray(win_times, dtype=float)
    if windows.shape[0] < 8:
        return float("nan")

    base_mask = win_times <= learn_end_time
    if not np.any(base_mask):
        return float("nan")
    Zb = windows[base_mask]
    tpl = np.mean(Zb, axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    else:
        return float("nan")

    # Score all windows
    for k in range(windows.shape[0]):
        c = _phase_invariant_corr(windows[k], tpl)
        corr.append(c)
        times.append(win_times[k])
    corr = np.asarray(corr, dtype=float)
    times = np.asarray(times, dtype=float)

    # Smooth lightly
    corr_s = _median_filter(corr, k=5)

    # Baseline stats
    mc, sc = _robust_mean_std(corr_s[base_mask])
    if not np.isfinite(mc) or not np.isfinite(sc):
        return float("nan")
    thr = min(mc - 4.0 * sc, 0.50)  # a bit less strict than cycle method

    # Sustained drop ~30% of window
    hold_windows = max(2, int(round(0.30 * (W / hop))))
    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    tA = _first_sustained_crossing_time(times, corr_s, thr, hold_windows, j0)
    if np.isfinite(tA):
        return float(tA)

    # Final resort: largest drop point post-baseline if it's clearly significant
    post_mask = np.arange(times.size) >= j0
    if not np.any(post_mask):
        return float("nan")
    post_corr = corr_s[post_mask]
    post_times = times[post_mask]
    if post_corr.size < 3:
        return float("nan")
    # Find minimum correlation after baseline
    jmin = int(np.argmin(post_corr))
    cmin = float(post_corr[jmin])
    drop_abs = float(mc - cmin)
    z_drop = drop_abs / max(sc, 1e-12)
    if drop_abs >= 0.25 and z_drop >= 3.0:
        return float(post_times[jmin])
    return float("nan")

# --------------------------------- Main ------------------------------------- #

def pattern_major_fault_time(signal_view) -> float:
    x, y = _as_numpy_xy(signal_view)
    if x.size < 256:
        return float("nan")

    # 1) Uniform resample
    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 256:
        return float("nan")

    # 2) Learn region
    learn_fraction = 0.30
    N = tu.size
    N_learn = min(max(256, int(N * learn_fraction)), max(256, N // 2))
    learn_end_time = float(tu[min(N_learn - 1, N - 1)])
    yL = yu[:N_learn]

    # 3) Try cycle-based method first
    T = _best_period(yL, dt)
    detected_times: List[float] = []

    if np.isfinite(T):
        spp = int(round(T / dt))
        if 12 <= spp <= max(16, N_learn // 2):
            min_dist = max(4, int(round(0.4 * spp)))
            peaks = _choose_peak_polarity(yu, min_dist)
            if peaks.size >= 6:
                segs = _cycle_segments(yu, peaks)
                if len(segs) >= 6:
                    cycles = [ yu[s:e] for (s, e) in segs ]
                    template, train_idxs = _build_template_from_cycles(cycles, max_train=12, L=128)
                    if template.size > 0 and len(train_idxs) >= 3:
                        # Score cycles
                        Ltpl = template.size
                        cycle_corr = []
                        for (s, e) in segs:
                            z = _resample_unit(yu[s:e], Ltpl)
                            cycle_corr.append(_phase_invariant_corr(z, template))
                        cycle_corr = np.asarray(cycle_corr, dtype=float)

                        base_corr = cycle_corr[train_idxs]
                        mc, sc = _robust_mean_std(base_corr)
                        if np.isfinite(mc) and np.isfinite(sc):
                            corr_thr = min(mc - 5.0 * sc, 0.45)
                            # Smooth a bit
                            corr_s = _median_filter(cycle_corr, k=3)
                            # Require two consecutive bad cycles for robustness
                            bad = corr_s < corr_thr
                            run = 0
                            j_star = None
                            last_train = max(train_idxs)
                            start_idx = last_train + 1
                            for j in range(start_idx, len(segs)):
                                if bad[j]:
                                    run += 1
                                    if run >= 2:
                                        j_star = j - 1
                                        break
                                else:
                                    run = 0
                            if j_star is not None:
                                t_refined = _refine_within_cycle_time(tu, yu, segs[j_star], template)
                                detected_times.append(float(t_refined))

    # 4) If cycle-based failed, use windowed fallback (no period needed)
    if not detected_times:
        t_fb = _window_shape_change_time(tu, yu, learn_end_time)
        if np.isfinite(t_fb):
            detected_times.append(float(t_fb))

    # 5) Return earliest detected time or NaN
    if not detected_times:
        return float("nan")
    return float(min(detected_times))

__all__ = ["pattern_major_fault_time"]
