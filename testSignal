"""
Robust single-timestamp detector for when a repeating signal pattern changes.

You pass ONLY:
- signal_view: object with .x (timestamps) and .y (values), or a tuple (x, y)

Returns:
- float timestamp (seconds) of the first MAJOR change, or float("nan") if none.

Design goals:
- Learn the nominal repeating pattern automatically from the first 30% (max half) of data.
- Be tolerant to minute variations; trigger only on significant, sustained changes.
- Combine multiple detectors:
  1) Pattern correlation drop vs learned template (shape change)
  2) CUSUM on mean and amplitude across windows (level/amplitude change)
- Earliest time from the above that passes strict, sustained criteria is returned.

No user-tunable parameters are exposed.
"""

from typing import Tuple
import numpy as np

# ------------------------------ Utilities ---------------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    # Accept (x, y) tuple or object with .x/.y
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)

    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")

    # Drop NaNs (synchronized)
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]

    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]

    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    """Resample to a uniform grid using linear interpolation. Returns (tu, yu, dt)."""
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    """Median and robust sigma via MAD (scaled). Falls back to std if needed."""
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

# ---------------------------- Period estimation ----------------------------- #

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via FFT of zero-mean data; NaN if not reliable."""
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0  # ignore DC
    # at least ~3 cycles in window and <= 90% Nyquist
    fmin = 3.0 / (N * dt)
    fmax = 0.45 / dt
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    k = np.argmax(mag[band])
    idx = np.flatnonzero(band)[k]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via autocorrelation peak; NaN if not reliable."""
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

# --------------------------- Template construction -------------------------- #

def _align_and_average(y: np.ndarray, spp: int, max_periods: int = 12, template_points: int = 128) -> np.ndarray:
    """
    Build a normalized template by aligning up to max_periods cycles (length spp),
    using circular shift to maximize correlation with the first cycle.
    """
    n = y.size
    spp = int(max(8, spp))
    K = min(max_periods, n // spp)
    if K < 2:
        return np.array([], dtype=float)

    ref = y[0:spp].astype(float)
    ref = ref - np.mean(ref)
    s = float(np.std(ref))
    if s > 0:
        ref /= s

    acc = np.zeros(spp, dtype=float)
    cnt = 0
    for i in range(K):
        seg = y[i * spp:(i + 1) * spp].astype(float)
        if seg.size != spp:
            break
        seg = seg - np.mean(seg)
        std = float(np.std(seg))
        if std > 0:
            seg = seg / std
        # align by circular shift maximizing correlation
        c = np.fft.ifft(np.fft.fft(ref) * np.conj(np.fft.fft(seg))).real
        shift = int(np.argmax(c)) % spp
        seg_aligned = np.roll(seg, shift)
        acc += seg_aligned
        cnt += 1

    if cnt == 0:
        return np.array([], dtype=float)

    avg = acc / cnt
    # resample to a fixed phase grid for stable comparison
    xi = np.linspace(0, 1, spp, endpoint=False)
    phase_grid = np.linspace(0, 1, template_points, endpoint=False)
    tpl = np.interp(phase_grid, xi, avg)
    # unit norm
    norm = float(np.linalg.norm(tpl))
    if norm > 0:
        tpl = tpl / norm
    return tpl

def _window_features(y: np.ndarray, i: int, spp: int, template: np.ndarray) -> Tuple[float, float, float, float]:
    """
    Features for window y[i:i+spp]:
    - correlation to template (normalized)
    - mean
    - std
    - peak-to-peak amplitude
    """
    seg = y[i:i + spp]
    if seg.size != spp:
        return np.nan, np.nan, np.nan, np.nan
    m = float(np.mean(seg))
    s = float(np.std(seg))
    a = float(np.max(seg) - np.min(seg))
    # normalized correlation
    z = seg - m
    if s > 0:
        z = z / s
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    seg_t = np.interp(np.linspace(0, 1, template.size, endpoint=False), xi, z)
    denom = float(np.linalg.norm(seg_t))
    c = float(np.dot(seg_t, template) / denom) if denom > 0 else 0.0
    return c, m, s, a

# ------------------------------- CUSUM -------------------------------------- #

def _cusum_first_time(series: np.ndarray, times: np.ndarray, baseline: float, sigma: float, k_sigma: float, h_sigma: float, start_idx: int) -> float:
    """
    One-sided two-direction CUSUM on a windowed feature series.
    Returns first time where either pos or neg CUSUM crosses h_sigma*sigma, else NaN.
    """
    if not np.isfinite(sigma) or sigma == 0:
        return float("nan")
    k = k_sigma * sigma
    h = h_sigma * sigma

    s_pos = 0.0
    s_neg = 0.0
    for j in range(start_idx, series.size):
        x = float(series[j]) - baseline
        s_pos = max(0.0, s_pos + (x - k))
        s_neg = min(0.0, s_neg + (x + k))
        if s_pos > h or abs(s_neg) > h:
            return float(times[j])
    return float("nan")

# -------------------------- Sample-level refinement ------------------------- #

def _period_diff_series(yu: np.ndarray, tu: np.ndarray, spp: int) -> Tuple[np.ndarray, np.ndarray]:
    """Compute |y[t] - y[t - spp]| and its timestamps."""
    if spp < 2 or yu.size <= spp:
        return np.array([], dtype=float), np.array([], dtype=float)
    d = np.abs(yu[spp:] - yu[:-spp])
    td = tu[spp:]
    # small median filter to reduce spikiness
    if d.size >= 5:
        k = 5
        pad = k // 2
        d_pad = np.pad(d, (pad, pad), mode="edge")
        d_smooth = np.median(
            np.stack([d_pad[i:i + d.size] for i in range(k)], axis=0), axis=0
        )
        d = d_smooth
    return td, d

def _refine_time_with_period_diff(
    tu: np.ndarray,
    yu: np.ndarray,
    spp: int,
    learn_end_time: float,
    t_init: float,
    md: float,
    sd: float,
    dt: float
) -> float:
    """Refine onset time around t_init using sample-level period-difference detector."""
    td, d = _period_diff_series(yu, tu, spp)
    if td.size == 0 or not np.isfinite(sd) or sd == 0:
        return float("nan")

    T = spp * dt
    t_lo = max(learn_end_time, t_init - 0.25 * T)
    t_hi = min(tu[-1], t_init + 1.25 * T)
    m = (td >= t_lo) & (td <= t_hi)
    if not np.any(m):
        return float("nan")

    thr = md + 4.0 * sd
    hold = max(3, int(round(0.10 * spp)))  # ~10% of a period

    run = 0
    for j in np.flatnonzero(m):
        if d[j] > thr:
            run += 1
            if run >= hold:
                t_ref = float(td[j - hold + 1])
                t_ref = max(t_ref, learn_end_time)
                t_ref = min(t_ref, t_init + T)
                return t_ref
        else:
            run = 0
    return float("nan")

# ------------------------------ Main detector ------------------------------- #

def pattern_change_time_robust(signal_view) -> float:
    """
    Return the first timestamp (float seconds) where a MAJOR pattern change occurs.
    Minor variations are ignored by design. Returns float('nan') if none detected.
    """
    x, y = _as_numpy_xy(signal_view)
    if x.size < 256:
        return float("nan")

    # Uniform resample
    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 256:
        return float("nan")

    # Learn baseline window
    learn_fraction = 0.30
    N = tu.size
    N_learn = min(max(256, int(N * learn_fraction)), max(256, N // 2))
    yL = yu[:N_learn]

    # Period estimate
    T = _best_period(yL, dt)
    if not np.isfinite(T):
        return float("nan")
    spp = int(round(T / dt))
    if spp < 12 or spp > N_learn // 2:
        return float("nan")

    # Template from learning region
    template = _align_and_average(yL, spp, max_periods=12, template_points=128)
    if template.size == 0:
        return float("nan")

    # Sliding features across full record
    hop = max(1, int(round(spp / 12)))  # finer hop for time resolution
    times = []
    corr = []
    meanv = []
    stdv = []
    ampv = []
    for i in range(0, N - spp, hop):
        c, m, s, a = _window_features(yu, i, spp, template)
        times.append(tu[i + spp // 2])  # center-of-window to avoid early bias
        corr.append(c)
        meanv.append(m)
        stdv.append(s)
        ampv.append(a)
    times = np.asarray(times, dtype=float)
    corr = np.asarray(corr, dtype=float)
    meanv = np.asarray(meanv, dtype=float)
    stdv = np.asarray(stdv, dtype=float)
    ampv = np.asarray(ampv, dtype=float)
    if times.size == 0:
        return float("nan")

    # Baseline stats (learn region)
    learn_end_time = tu[min(N_learn - 1, N - 1)]
    learn_mask = times <= learn_end_time
    if not np.any(learn_mask):
        return float("nan")

    mc, sc = _robust_mean_std(corr[learn_mask])
    mm, sm = _robust_mean_std(meanv[learn_mask])
    ms, ss = _robust_mean_std(stdv[learn_mask])
    ma, sa = _robust_mean_std(ampv[learn_mask])
    if not all(np.isfinite(v) for v in (mc, sc, mm, sm, ms, ss, ma, sa)):
        return float("nan")

    # Detector A: correlation drop (shape change), with gating by period-diff confirmation
    corr_enabled = np.isfinite(mc) and np.isfinite(sc) and mc >= 0.70  # skip A if baseline corr is weak
    tA = float("nan")
    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    if corr_enabled:
        corr_thr = min(mc - 3.5 * sc, 0.70)  # responsive but conservative
        hold_windows = max(3, int(round(0.30 * spp / hop)))  # ~30% period sustained
        bad_corr = corr < corr_thr

        # Prepare period-diff baseline for confirmation
        td_all, d_all = _period_diff_series(yu, tu, spp)
        md = sd = float("nan")
        if td_all.size > 0:
            m_d_learn = td_all <= learn_end_time
            if np.any(m_d_learn):
                md, sd = _robust_mean_std(d_all[m_d_learn])

        j = j0
        run = 0
        while j < times.size:
            if bad_corr[j]:
                run += 1
                if run >= hold_windows:
                    jA_start = j - hold_windows + 1
                    # Interpolate crossing time
                    k = jA_start - 1
                    if k >= j0 and np.isfinite(corr[k]):
                        y1, t1 = corr[k], times[k]
                        y2, t2 = corr[jA_start], times[jA_start]
                        if y2 != y1:
                            alpha = (corr_thr - y1) / (y2 - y1)
                            alpha = float(np.clip(alpha, 0.0, 1.0))
                            t_cross = t1 + alpha * (t2 - t1)
                        else:
                            t_cross = times[jA_start]
                    else:
                        t_cross = times[jA_start]

                    # Confirm with period-diff; if cannot confirm, continue scanning
                    t_ref = float("nan")
                    if np.isfinite(md) and np.isfinite(sd) and sd > 0:
                        t_ref = _refine_time_with_period_diff(tu, yu, spp, learn_end_time, t_cross, md, sd, dt)
                    if np.isfinite(t_ref):
                        tA = t_ref
                        break
                    else:
                        # discard this correlation run as spurious and continue
                        run = 0
            else:
                run = 0
            j += 1

    # Detector B: CUSUM on window mean (level shift)
    tB = _cusum_first_time(meanv, times, mm, sm, k_sigma=0.5, h_sigma=7.0, start_idx=j0)

    # Detector C: CUSUM on window amplitude (pulse height/width changes)
    tC = _cusum_first_time(ampv, times, ma, sa, k_sigma=0.5, h_sigma=7.0, start_idx=j0)

    # Choose earliest valid time
    candidates = [t for t in (tA, tB, tC) if np.isfinite(t)]
    if not candidates:
        return float("nan")
    return float(max(min(candidates), learn_end_time))

__all__ = ["pattern_change_time_robust"]
