"""
Detect the timestamp of a MAJOR disturbance in a repeating signal pattern.

What this version does to avoid returning NaN and mis-timing:
- Shape-only comparisons: each window is z-normalized (offset and amplitude ignored).
- Phase-invariant: circular correlation handles small timing jitter within a cycle.
- Stable-template learning: uses only the most self-consistent baseline windows.
- Pre-change stability check: requires high correlation immediately before the drop.
- Sustained low state: requires the low-correlation state to persist (~1â€“2 periods).
- Multiple fallbacks so you still get a timestamp instead of NaN when possible.

Public API
- pattern_major_fault_time(signal_view) -> float
  signal_view: object with .x (timestamps) and .y (values), or a tuple (x, y)
  returns: first timestamp (seconds) of a major shape change, or float("nan")
"""

from typing import Tuple, List
import numpy as np

# ------------------------------ Utilities ---------------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)
    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")
    # Drop NaNs, synchronized
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]
    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]
    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _median_filter(v: np.ndarray, k: int = 5) -> np.ndarray:
    if k <= 1 or v.size == 0:
        return v.copy()
    if k % 2 == 0:
        k += 1
    r = k // 2
    pad = np.pad(v, (r, r), mode="reflect")
    out = np.empty_like(v)
    for i in range(v.size):
        out[i] = np.median(pad[i:i+k])
    return out

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

# ---------------------------- Period estimation ----------------------------- #

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0
    fmin = 3.0 / (N * dt)
    fmax = 0.45 / dt
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    k = int(np.argmax(mag[band]))
    idx = np.flatnonzero(band)[k]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

# --------------------------- Shape-only template ---------------------------- #

def _resample_unit(seg: np.ndarray, L: int) -> np.ndarray:
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    z = seg - np.mean(seg)
    s = float(np.std(seg))
    if s > 0:
        z = z / s
    else:
        z = z * 0.0
    return np.interp(np.linspace(0, 1, L, endpoint=False), xi, z)

def _phase_invariant_corr(a: np.ndarray, b: np.ndarray) -> float:
    # a, b already z-normalized shapes in the same length space
    na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))
    if na == 0.0 or nb == 0.0:
        return 0.0
    a = a / na; b = b / nb
    c = np.fft.ifft(np.fft.fft(a) * np.conj(np.fft.fft(b))).real
    return float(np.clip(np.max(c), -1.0, 1.0))

def _template_from_learning(yu: np.ndarray, spp: int, L: int, learn_samples: int) -> np.ndarray:
    # Build windows over the learning region and keep the most consistent ones
    hop = max(1, spp // 8)
    windows = []
    for i in range(0, learn_samples - spp, hop):
        windows.append(_resample_unit(yu[i:i+spp], L))
    if len(windows) < 6:
        return np.array([], dtype=float)
    W = np.stack(windows, axis=0)
    # Initial template = mean
    tpl = np.mean(W, axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    # Score windows by correlation to initial template
    scores = np.array([_phase_invariant_corr(w, tpl) for w in W], dtype=float)
    # Keep top K consistent windows
    order = np.argsort(-scores)
    K = min(16, max(8, len(order)//2))
    top = W[order[:K]]
    tpl = np.mean(top, axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    return tpl

# ------------------------------- Detection ---------------------------------- #

def _correlation_series(tu: np.ndarray, yu: np.ndarray, spp: int, L: int, template: np.ndarray) -> Tuple[np.ndarray, np.ndarray, int]:
    hop = max(1, spp // 8)  # ~8 hops per period
    times = []
    corr = []
    N = yu.size
    for i in range(0, N - spp, hop):
        seg = yu[i:i+spp]
        z = _resample_unit(seg, L)
        c = _phase_invariant_corr(z, template)
        times.append(tu[i + spp // 2])
        corr.append(c)
    return np.asarray(times, dtype=float), np.asarray(corr, dtype=float), hop

def _interpolated_first_drop_strict(times: np.ndarray, corrs: np.ndarray, learn_end_time: float, mc: float, sc: float, spp: int, hop: int) -> float:
    # Threshold: big drop and also low absolute correlation
    thr = min(mc - 5.0 * sc, 0.40)
    # Persistence: ~1.5 periods below threshold (helps avoid early detections)
    hold_after = max(4, int(round(1.5 * spp / hop)))
    # Pre-stability: ~0.75 periods of high correlation before the change
    pre_ok_windows = max(3, int(round(0.75 * spp / hop)))
    pre_thr = max(0.72, mc - 1.0 * sc)

    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    below = corrs < thr

    # Scan for a run of low correlation preceded by high/stable correlation
    run = 0
    for j in range(j0, len(times)):
        if below[j]:
            run += 1
        else:
            run = 0
        if run >= hold_after:
            s = j - run + 1  # start index of low-corr run
            # Check pre-stability just before s
            p0 = max(0, s - pre_ok_windows)
            if p0 < s and np.all(corrs[p0:s] > pre_thr):
                # Interpolate first threshold crossing between last high and first low
                k = s - 1
                while k >= j0 and below[k]:
                    k -= 1
                if k < j0:
                    return float(times[s])
                y0, y1 = float(corrs[k]), float(corrs[k+1])
                t0, t1 = float(times[k]), float(times[k+1])
                if y1 == y0 or t1 == t0:
                    return float(times[s])
                alpha = (thr - y0) / (y1 - y0)
                alpha = float(np.clip(alpha, 0.0, 1.0))
                return float(t0 + alpha * (t1 - t0))
    return float("nan")

# ---------------------------- Windowed fallback (no T) ---------------------- #

def _window_shape_change_time(tu: np.ndarray, yu: np.ndarray, learn_end_time: float) -> float:
    """
    Fallback: detect shape change without relying on an accurate period.
    Returns earliest time where correlation to a baseline window-template drops and stays low.
    """
    N = tu.size
    if N < 128:
        return float("nan")

    # Window ~ 5% of record (clipped), hop quarter window
    W = int(np.clip(N // 20, 64, N // 6))
    hop = max(1, W // 4)
    L = 128  # template length

    # Build baseline windows (<= learn_end_time)
    windows = []
    win_times = []
    for i in range(0, N - W, hop):
        seg = yu[i:i+W]
        z = _resample_unit(seg, L)
        windows.append(z)
        win_times.append(tu[i + W // 2])
    windows = np.asarray(windows, dtype=float)
    win_times = np.asarray(win_times, dtype=float)
    if windows.shape[0] < 8:
        return float("nan")

    base_mask = win_times <= learn_end_time
    if not np.any(base_mask):
        return float("nan")
    Zb = windows[base_mask]
    tpl = np.mean(Zb, axis=0)
    n = float(np.linalg.norm(tpl))
    if n > 0:
        tpl = tpl / n
    else:
        return float("nan")

    # Score all windows
    corr = np.array([_phase_invariant_corr(w, tpl) for w in windows], dtype=float)
    times = win_times.copy()

    # Smooth lightly
    corr_s = _median_filter(corr, k=5)

    # Baseline stats
    mc, sc = _robust_mean_std(corr_s[base_mask])
    if not np.isfinite(mc) or not np.isfinite(sc):
        return float("nan")
    thr = min(mc - 4.0 * sc, 0.50)

    # Sustained drop ~30% of window
    hold_windows = max(2, int(round(0.30 * (W / hop))))
    j0 = int(np.searchsorted(times, learn_end_time, side="right"))

    # Pre-stability: require several windows just before drop to be high
    pre_ok = max(3, int(round(0.50 * (W / hop))))
    pre_thr = max(0.70, mc - 1.0 * sc)

    below = corr_s < thr
    run = 0
    for j in range(j0, times.size):
        if below[j]:
            run += 1
        else:
            run = 0
        if run >= hold_windows:
            s = j - run + 1
            p0 = max(0, s - pre_ok)
            if p0 < s and np.all(corr_s[p0:s] > pre_thr):
                # interpolate crossing between last high and first low
                k = s - 1
                while k >= j0 and below[k]:
                    k -= 1
                if k < j0:
                    return float(times[s])
                y0, y1 = float(corr_s[k]), float(corr_s[k+1])
                t0, t1 = float(times[k]), float(times[k+1])
                if y1 == y0 or t1 == t0:
                    return float(times[s])
                alpha = (thr - y0) / (y1 - y0)
                alpha = float(np.clip(alpha, 0.0, 1.0))
                return float(t0 + alpha * (t1 - t0))

    # Last resort: biggest post-baseline negative jump that stays low afterwards
    post_mask = np.arange(times.size) >= j0
    if np.any(post_mask):
        diffs = np.diff(corr_s[post_mask])
        if diffs.size > 0:
            jm = int(np.argmin(diffs))  # largest drop
            idx = j0 + jm + 1
            tail_low = np.mean(corr_s[idx: idx + max(3, hold_windows)]) < (mc - 2.5 * sc)
            if tail_low:
                return float(times[idx])

    return float("nan")

# --------------------------------- Main ------------------------------------- #

def pattern_major_fault_time(signal_view) -> float:
    x, y = _as_numpy_xy(signal_view)
    if x.size < 128:
        return float("nan")

    # 1) Uniform resample
    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 128:
        return float("nan")

    # 2) Learn window (~30% but max half)
    N = tu.size
    N_learn = min(max(128, int(0.30 * N)), max(128, N // 2))
    learn_end_time = float(tu[min(N_learn - 1, N - 1)])
    yL = yu[:N_learn]

    # 3) Period estimate on learn window
    T = _best_period(yL, dt)
    if np.isfinite(T):
        spp = int(round(T / dt))
        if 12 <= spp <= max(16, N_learn // 2):
            # Template from baseline region
            L = 128
            template = _template_from_learning(yu, spp, L, learn_samples=N_learn)
            if template.size > 0:
                times, corr, hop = _correlation_series(tu, yu, spp, L, template)
                if times.size > 0:
                    corr_s = _median_filter(corr, k=5)
                    learn_mask = times <= learn_end_time
                    if np.any(learn_mask):
                        mc, sc = _robust_mean_std(corr_s[learn_mask])
                        if np.isfinite(mc) and np.isfinite(sc):
                            t = _interpolated_first_drop_strict(times, corr_s, learn_end_time, mc, sc, spp, hop)
                            if np.isfinite(t):
                                return float(t)

    # 4) Fallback: windowed detection without accurate period
    t_fb = _window_shape_change_time(tu, yu, learn_end_time)
    if np.isfinite(t_fb):
        return float(t_fb)

    return float("nan")

__all__ = ["pattern_major_fault_time"]
