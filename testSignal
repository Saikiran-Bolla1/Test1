"""
Signal change detectors.

Includes:
- burst_time: Detects the start/end timestamp of a burst of dense threshold crossings.
- pattern_major_fault_time: Detects the first MAJOR disturbance time in a repeating pattern.

No user-tunable parameters are exposed beyond the required function arguments.
"""

from typing import Tuple, Literal
import numpy as np

Edge = Literal["rising", "falling"]

# ============================== Common utilities ==============================

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    """
    Return x, y as numpy arrays, drop NaNs in sync, and ensure time is strictly ascending.
    If there are duplicate timestamps, they are stably sorted and kept as-is.
    """
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)

    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")

    # Drop NaNs (synchronized)
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]

    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x, kind="stable")
        x, y = x[idx], y[idx]

    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Resample to a uniform grid using linear interpolation.
    Returns (tu, yu, dt). If dt is not finite or too small, returns (x, y, inf).
    """
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    """
    Robust median and sigma via MAD (scaled). Falls back to std if needed.
    """
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

def _interp_cross_time(x0: float, x1: float, y0: float, y1: float, thr: float) -> float:
    """Linear interpolation for threshold crossing at level thr between (x0,y0) and (x1,y1)."""
    if y1 == y0:
        return float(x0)
    return float(x0 + (x1 - x0) * (thr - y0) / (y1 - y0))


# ============================== Burst detector ===============================

def _schmitt_edges(signal_view, threshold: float, atol: float, edge: Edge) -> np.ndarray:
    """
    Schmitt-trigger style edge extractor with hysteresis band [thr-atol, thr+atol].
    - For 'rising', records times when the state flips from BELOW to ABOVE.
    - For 'falling', records times when the state flips from ABOVE to BELOW.
    Uses linear interpolation to timestamp the crossing at 'threshold'.
    """
    x, y = _as_numpy_xy(signal_view)
    n = x.size
    if n < 2:
        return np.array([], dtype=float)

    thr = float(threshold)
    up = thr + float(atol)
    dn = thr - float(atol)

    # Initial state
    above = y[0] > up
    below = y[0] < dn
    state = 1 if above else (0 if below else None)

    times = []
    for i in range(n - 1):
        y0, y1 = float(y[i]), float(y[i + 1])
        x0, x1 = float(x[i]), float(x[i + 1])

        # Update state only after crossing the opposite bound
        if state is None:
            # Inside deadband: decide if we left it
            if y1 >= up:
                state = 1
                if edge == "rising":
                    times.append(_interp_cross_time(x0, x1, y0, y1, thr))
            elif y1 <= dn:
                state = 0
                if edge == "falling":
                    times.append(_interp_cross_time(x0, x1, y0, y1, thr))
            continue

        if state == 0:
            # Currently below; look for rising transition only if y crosses 'up'
            if y0 <= dn and y1 >= up:
                state = 1
                if edge == "rising":
                    times.append(_interp_cross_time(x0, x1, y0, y1, thr))
            elif y0 <= dn and dn < y1 < up:
                # Enter deadband upward: wait until leaving at 'up'
                if y1 >= up:
                    state = 1
                    if edge == "rising":
                        times.append(_interp_cross_time(x0, x1, y0, y1, thr))
        else:
            # state == 1: currently above; look for falling transition only if y crosses 'dn'
            if y0 >= up and y1 <= dn:
                state = 0
                if edge == "falling":
                    times.append(_interp_cross_time(x0, x1, y0, y1, thr))
            elif y0 >= up and dn < y1 < up:
                # Enter deadband downward: wait until leaving at 'dn'
                if y1 <= dn:
                    state = 0
                    if edge == "falling":
                        times.append(_interp_cross_time(x0, x1, y0, y1, thr))

    if not times:
        return np.array([], dtype=float)
    return np.array(times, dtype=float)

def _dense_runs_from_edges(edges: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Identify dense toggling runs from edge times using an adaptive gap threshold.
    Returns (run_starts, run_ends, gap_threshold) where starts/ends are indices into gaps.
    A run of gaps[i0..i1] corresponds to edges[i0]..edges[i1+1].
    """
    if edges.size < 4:
        return np.array([], dtype=int), np.array([], dtype=int), float("nan")

    gaps = np.diff(edges)
    if gaps.size == 0:
        return np.array([], dtype=int), np.array([], dtype=int), float("nan")

    med_all = float(np.median(gaps))
    p25_all = float(np.percentile(gaps, 25))
    # Use early portion as baseline if available
    k_learn = max(5, int(0.3 * gaps.size))
    med_base = float(np.median(gaps[:k_learn])) if gaps.size >= k_learn else med_all

    # Adaptive threshold favors dense (short) gaps; conservative relative to baseline
    thr = max(1e-12, min(0.6 * med_all, 1.2 * p25_all, 0.8 * med_base))

    small = gaps <= thr
    if not np.any(small):
        return np.array([], dtype=int), np.array([], dtype=int), thr

    # Find contiguous True segments in 'small'
    b = small.astype(np.int8)
    db = np.diff(b)
    # Starts are where we go 0->1; also include if first is True
    starts = np.flatnonzero(np.concatenate(([b[0] == 1], db == 1)))
    # Ends are where we go 1->0; also include if last is True
    ends = np.flatnonzero(np.concatenate((db == -1, [b[-1] == 1])))

    # Keep only runs with at least 3 small gaps (>= 4 edges in the run)
    valid = (ends - starts + 1) >= 3
    return starts[valid], ends[valid], thr

def burst_time(
    signal_view,
    value: float,
    atol: float,
    edge: Edge,
    burst: bool = True,  # kept for signature clarity; always treated as burst detection
) -> float:
    """
    Single-timestamp burst detector.

    - edge='rising'  -> returns the burst START time (first edge of dense run)
    - edge='falling' -> returns the burst END time   (last edge of dense run)

    Returns float('nan') if not detected.
    """
    edges = _schmitt_edges(signal_view, value, atol, edge)
    if edges.size < 4:
        return float("nan")

    run_starts, run_ends, _thr = _dense_runs_from_edges(edges)
    if run_starts.size == 0:
        return float("nan")

    if edge == "rising":
        # Choose earliest dense run (start of burst)
        i0 = int(run_starts[0])
        return float(edges[i0])
    else:
        # Choose latest dense run (end of burst)
        i1 = int(run_ends[-1]) + 1  # convert gap-index to edge index
        i1 = min(i1, edges.size - 1)
        return float(edges[i1])


# ===================== Major repeating pattern change detector =====================

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via FFT of zero-mean data; NaN if not reliable."""
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0  # ignore DC
    fmin = 3.0 / (N * dt)   # at least ~3 cycles in learn window
    fmax = 0.45 / dt        # <= 90% Nyquist
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    idx = np.flatnonzero(band)[np.argmax(mag[band])]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via autocorrelation peak; NaN if not reliable."""
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

def _align_and_average(y: np.ndarray, spp: int, max_periods: int = 12, template_points: int = 128) -> np.ndarray:
    """
    Build a normalized template by aligning up to max_periods cycles (length spp),
    using circular shift to maximize correlation with the first cycle.
    """
    n = y.size
    spp = int(max(8, spp))
    K = min(max_periods, n // spp)
    if K < 2:
        return np.array([], dtype=float)

    ref = y[0:spp].astype(float)
    ref = ref - np.mean(ref)
    s = float(np.std(ref))
    if s > 0:
        ref /= s

    acc = np.zeros(spp, dtype=float)
    cnt = 0
    for i in range(K):
        seg = y[i * spp:(i + 1) * spp].astype(float)
        if seg.size != spp:
            break
        seg = seg - np.mean(seg)
        std = float(np.std(seg))
        if std > 0:
            seg = seg / std
        c = np.fft.ifft(np.fft.fft(ref) * np.conj(np.fft.fft(seg))).real
        shift = int(np.argmax(c)) % spp
        acc += np.roll(seg, shift)
        cnt += 1

    if cnt == 0:
        return np.array([], dtype=float)

    avg = acc / cnt
    xi = np.linspace(0, 1, spp, endpoint=False)
    phase_grid = np.linspace(0, 1, template_points, endpoint=False)
    tpl = np.interp(phase_grid, xi, avg)
    norm = float(np.linalg.norm(tpl))
    if norm > 0:
        tpl = tpl / norm
    return tpl

def _window_features(y: np.ndarray, i: int, spp: int, template: np.ndarray) -> Tuple[float, float, float, float]:
    """
    Features for window y[i:i+spp]:
    - correlation to template (normalized)
    - mean
    - std
    - peak-to-peak amplitude
    """
    seg = y[i:i + spp]
    if seg.size != spp:
        return np.nan, np.nan, np.nan, np.nan
    m = float(np.mean(seg))
    s = float(np.std(seg))
    a = float(np.max(seg) - np.min(seg))
    # normalized correlation
    z = seg - m
    if s > 0:
        z = z / s
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    seg_t = np.interp(np.linspace(0, 1, template.size, endpoint=False), xi, z)
    denom = float(np.linalg.norm(seg_t))
    c = float(np.dot(seg_t, template) / denom) if denom > 0 else 0.0
    return c, m, s, a

def _period_diff_series(yu: np.ndarray, tu: np.ndarray, spp: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    Sample-level period-difference detector: d[t] = |y[t] - y[t - spp]|.
    Returns (td, d); td aligns with yu[spp:].
    """
    if spp < 2 or yu.size <= spp:
        return np.array([], dtype=float), np.array([], dtype=float)
    d = np.abs(yu[spp:] - yu[:-spp])
    td = tu[spp:]
    # light median smoothing (k=5) to reduce spikiness
    if d.size >= 5:
        k = 5
        pad = k // 2
        dp = np.pad(d, (pad, pad), mode="edge")
        d = np.median(np.stack([dp[i:i + d.size] for i in range(k)], axis=0), axis=0)
    return td, d

def _refine_time_with_period_diff(
    tu: np.ndarray,
    yu: np.ndarray,
    spp: int,
    learn_end_time: float,
    t_init: float,
    md: float,
    sd: float,
    dt: float,
) -> float:
    """
    Refine coarse trigger time using the first sustained crossing on d[t] near t_init.
    Returns NaN if refinement cannot confirm a nearby onset.
    """
    td, d = _period_diff_series(yu, tu, spp)
    if td.size == 0 or not np.isfinite(sd) or sd == 0:
        return float("nan")

    T = spp * dt
    t_lo = max(learn_end_time, t_init - 0.25 * T)
    t_hi = min(tu[-1], t_init + 1.25 * T)
    m = (td >= t_lo) & (td <= t_hi)
    if not np.any(m):
        return float("nan")

    thr = md + 4.0 * sd  # strict above-baseline threshold
    hold = max(3, int(round(0.10 * spp)))  # ~10% of a period

    run = 0
    for j in np.flatnonzero(m):
        if d[j] > thr:
            run += 1
            if run >= hold:
                t_ref = float(td[j - hold + 1])
                t_ref = max(t_ref, learn_end_time)
                t_ref = min(t_ref, t_init + T)
                return t_ref
        else:
            run = 0
    return float("nan")

def pattern_major_fault_time(signal_view) -> float:
    """
    Return the first timestamp (float seconds) where a MAJOR disturbance occurs.
    Minor changes are ignored by design. Returns float('nan') if none detected.

    Strategy:
    - Learn template from first 30% (max half).
    - Slide a window of one period with hop ~T/12; use center-of-window timestamps.
    - Detect:
      A) Large correlation drop (c < min(mc - 4Ïƒ, 0.55)), sustained ~35% of a period,
         then refine onset with sample-level period-diff and require confirmation.
      B) CUSUM-like via sigma test on mean/std/amp: require at least two z-scores > 6,
         sustained ~35% of a period.
    - Return the earliest confirmed time, clamped to not earlier than learn_end_time.
    """
    x, y = _as_numpy_xy(signal_view)
    if x.size < 256:
        return float("nan")

    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 256:
        return float("nan")

    # Learn region
    learn_fraction = 0.30
    N = tu.size
    N_learn = min(max(256, int(N * learn_fraction)), max(256, N // 2))
    yL = yu[:N_learn]
    learn_end_time = tu[min(N_learn - 1, N - 1)]

    # Period estimate
    T = _best_period(yL, dt)
    if not np.isfinite(T):
        return float("nan")
    spp = int(round(T / dt))
    if spp < 12 or spp > N_learn // 2:
        return float("nan")

    # Template
    template = _align_and_average(yL, spp, max_periods=12, template_points=128)
    if template.size == 0:
        return float("nan")

    # Sliding features
    hop = max(1, int(round(spp / 12)))  # ~12 hops per period for better time resolution
    times = []
    corr = []
    meanv = []
    stdv = []
    ampv = []
    for i in range(0, N - spp, hop):
        c, m, s, a = _window_features(yu, i, spp, template)
        times.append(tu[i + spp // 2])  # center-of-window to avoid early bias
        corr.append(c)
        meanv.append(m)
        stdv.append(s)
        ampv.append(a)

    times = np.asarray(times, dtype=float)
    corr = np.asarray(corr, dtype=float)
    meanv = np.asarray(meanv, dtype=float)
    stdv = np.asarray(stdv, dtype=float)
    ampv = np.asarray(ampv, dtype=float)
    if times.size == 0:
        return float("nan")

    learn_mask = times <= learn_end_time
    if not np.any(learn_mask):
        return float("nan")

    mc, sc = _robust_mean_std(corr[learn_mask])
    mm, sm = _robust_mean_std(meanv[learn_mask])
    ms, ss = _robust_mean_std(stdv[learn_mask])
    ma, sa = _robust_mean_std(ampv[learn_mask])
    if not all(np.isfinite(v) for v in (mc, sc, mm, sm, ms, ss, ma, sa)):
        return float("nan")

    # Detector A: correlation drop (shape change), sustained, with interpolation and refinement
    c_thr = min(mc - 4.0 * sc, 0.55)  # conservative absolute cap
    hold_windows = max(3, int(round(0.35 * spp / hop)))  # ~35% of a period sustained
    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    bad = corr < c_thr

    # Precompute period-diff baseline for refinement
    td_all, d_all = _period_diff_series(yu, tu, spp)
    md = sd = float("nan")
    if td_all.size > 0:
        m_d_learn = td_all <= learn_end_time
        if np.any(m_d_learn):
            md, sd = _robust_mean_std(d_all[m_d_learn])

    tA = float("nan")
    run = 0
    for j in range(j0, times.size):
        if bad[j]:
            run += 1
            if run >= hold_windows:
                j_start = j - hold_windows + 1
                k = j_start - 1
                # Linear interpolate the threshold crossing between last good and first bad
                if k >= j0 and np.isfinite(corr[k]):
                    y1, t1 = corr[k], times[k]
                    y2, t2 = corr[j_start], times[j_start]
                    if y2 != y1:
                        alpha = (c_thr - y1) / (y2 - y1)
                        alpha = float(np.clip(alpha, 0.0, 1.0))
                        t_cross = t1 + alpha * (t2 - t1)
                    else:
                        t_cross = times[j_start]
                else:
                    t_cross = times[j_start]

                # Confirm and refine with period-diff
                t_ref = float("nan")
                if np.isfinite(md) and np.isfinite(sd) and sd > 0:
                    t_ref = _refine_time_with_period_diff(tu, yu, spp, learn_end_time, t_cross, md, sd, dt)
                if np.isfinite(t_ref):
                    tA = t_ref
                    break
                else:
                    # If not confirmed, continue scanning
                    run = 0
        else:
            run = 0

    # Detector B: feature sigma test (mean/std/amp), sustained
    z_mean = np.abs(meanv - mm) / max(sm, 1e-12)
    z_std  = np.abs(stdv - ms)  / max(ss, 1e-12)
    z_amp  = np.abs(ampv - ma)  / max(sa, 1e-12)
    major_feat = ((z_mean > 6.0).astype(int) + (z_std > 6.0).astype(int) + (z_amp > 6.0).astype(int)) >= 2

    tB = float("nan")
    run = 0
    for j in range(j0, times.size):
        if major_feat[j]:
            run += 1
            if run >= hold_windows:
                tB = float(times[j - hold_windows + 1])
                break
        else:
            run = 0

    # Final selection (earliest confirmed)
    candidates = [t for t in (tA, tB) if np.isfinite(t)]
    if not candidates:
        return float("nan")
    return float(max(min(candidates), learn_end_time))


__all__ = [
    "burst_time",
    "pattern_major_fault_time",
]
