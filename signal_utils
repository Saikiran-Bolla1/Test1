"""
Signal analysis helpers updated to return a single float timestamp for any API that outputs a time.

Conventions for time-returning functions:
- Always return a Python float (seconds).
- If an event is not found, return float("nan") instead of None or arrays.

Provided features:
- Robust edge detection (first edge only as public API).
- Step start detection (stable low -> crossing -> stable high).
- Burst start detection (first edge after a quiet gap with optional confirmation).
- Interpolation helpers (value at time, time at value).
- Misc utilities (persistent value time, pattern match time, trends, stuck start time).
- In-place scaling helper for recorder results (_SignalView-like objects).

Expected 'signal_view' interface:
- signal_view.x: 1D array-like of timestamps (preferably increasing).
- signal_view.y: 1D array-like of values (same length as x).

Author: GitHub Copilot (updated per "single float timestamp" requirement)
"""

from typing import List, Literal, Tuple, Union, Optional
import numpy as np

Edge = Literal["rising", "falling"]

# ---------------------------- Internal utilities ---------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    """Return x, y as numpy arrays and ensure they are sorted by time ascending."""
    # Accept either a (x, y) tuple or an object with .x and .y
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(signal_view.x, dtype=float)
        y = np.asarray(signal_view.y, dtype=float)

    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")

    # Ensure increasing order in time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x = x[idx]
        y = y[idx]
    return x, y

def _interp_cross_time(x0: float, x1: float, y0: float, y1: float, thr: float) -> float:
    """Linear interpolation to find the crossing time at threshold thr between two points."""
    if y1 == y0:
        return float(x0)
    return float(x0 + (x1 - x0) * (thr - y0) / (y1 - y0))

# ------------------------- Interpolation convenience ------------------------ #

def gettimestampbyvalue(signal_view, value: float) -> float:
    """
    Returns the first timestamp where the signal crosses a given 'value' as a float.
    - If the value is exactly present, returns its first occurrence.
    - Otherwise, uses linear interpolation between the two points around the crossing.
    - Returns NaN if no crossing found or value is outside the signal's range.
    """
    x, y = _as_numpy_xy(signal_view)
    if y.size < 2 or value < np.min(y) or value > np.max(y):
        return float("nan")

    # exact match first
    exact = np.flatnonzero(y == value)
    if exact.size > 0:
        return float(x[int(exact[0])])

    # sign change around value
    indices = np.flatnonzero(np.diff(np.sign(y - value)) != 0)
    if indices.size == 0:
        return float("nan")

    i = int(indices[0])
    return _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], value)

def getvaluebytimestamp(signal_view, timestamp: float) -> Optional[float]:
    """
    Returns the signal value at a given 'timestamp' (linear interpolation).
    - If the timestamp is exactly present, returns the exact sample value.
    - Otherwise, linearly interpolates using the two neighboring timestamps.
    - Returns None if timestamp is outside [min(x), max(x)] or x has fewer than 2 samples.
    Note: This function returns a value (not a time), so it keeps Optional[float].
    """
    x, y = _as_numpy_xy(signal_view)
    if x.size < 2 or timestamp < x[0] or timestamp > x[-1]:
        return None

    # Exact match?
    exact = np.flatnonzero(x == timestamp)
    if exact.size > 0:
        return float(y[int(exact[0])])

    # Locate insertion point
    j = np.searchsorted(x, timestamp)
    if j == 0 or j == x.size:
        return None  # out of bounds safeguard
    i = j - 1
    x0, x1 = x[i], x[i + 1]
    y0, y1 = y[i], y[i + 1]
    if x1 == x0:
        return float(y0)
    v = y0 + (y1 - y0) * (timestamp - x0) / (x1 - x0)
    return float(v)

# ------------------------------- Edge finding ------------------------------- #

def _interp_cross_time(x0: float, x1: float, y0: float, y1: float, thr: float) -> float:
    # Linear interpolation for the crossing at threshold thr
    if y1 == y0:
        return float(x0)
    return float(x0 + (x1 - x0) * (thr - y0) / (y1 - y0))

def find_edge_time(signal_view, value: float, atol: float, edge: Edge) -> float:
    """
    First edge crossing time at 'value' with direction 'edge'.
    Returns float('nan') if not found.
    """
    x, y = _as_numpy_xy(signal_view)
    n = y.size
    if n < 2:
        return float("nan")

    thr = float(value)
    tol = float(atol)
    thr_up = thr + tol
    thr_dn = thr - tol

    # A) Hysteresis crossing
    if edge == "rising":
        mask = (y[:-1] <= thr_dn) & (y[1:] >= thr_up)
    elif edge == "falling":
        mask = (y[:-1] >= thr_up) & (y[1:] <= thr_dn)
    else:
        raise ValueError("edge must be 'rising' or 'falling'")

    idxs = np.flatnonzero(mask)
    if idxs.size > 0:
        i = int(idxs[0])
        return _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], thr)

    # B) Exact-sample match within tolerance
    exact = np.flatnonzero(np.abs(y - thr) <= tol)
    if exact.size > 0:
        return float(x[int(exact[0])])

    # C) Non-hysteresis crossing (touching counts)
    if edge == "rising":
        mask2 = (y[:-1] <= thr) & (y[1:] >= thr)
    else:  # falling
        mask2 = (y[:-1] >= thr) & (y[1:] <= thr)

    idxs2 = np.flatnonzero(mask2)
    if idxs2.size > 0:
        i = int(idxs2[0])
        return _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], thr)

    return float("nan")

def time_between_signals(
    signal_view1,
    signal_view2,
    value1: float,
    value2: float,
    edge1: Edge = "rising",
    edge2: Edge = "rising",
    atol1: float = 1e-8,
    atol2: float = 1e-8
) -> float:
    """
    Returns the absolute time difference between the first edge crossings
    in signal_view1 and signal_view2 at the specified thresholds and edge directions.
    Returns NaN if either crossing is not found.
    """
    t1 = find_first_edge_time(signal_view1, value1, edge=edge1, atol=atol1)
    t2 = find_first_edge_time(signal_view2, value2, edge=edge2, atol=atol2)
    if np.isnan(t1) or np.isnan(t2):
        return float("nan")
    return float(abs(t2 - t1))

# ------------------------ Change-start specialized finders ------------------- #

def _auto_threshold_from_ends(y: np.ndarray) -> float:
    """Estimate a mid-level threshold from the first and last 10% (or at least 5 samples) of y."""
    n = y.size
    if n == 0:
        return 0.0
    w = max(5, n // 10)
    low_med = float(np.median(y[:w]))
    high_med = float(np.median(y[-w:]))
    return 0.5 * (low_med + high_med)

def find_step_start(
    signal_view,
    threshold: Optional[float] = None,
    edge: Edge = "rising",
    min_low_samples: int = 3,
    min_high_samples: int = 3,
    atol: float = 1e-8
) -> float:
    """
    Detects the start time of a stable step change and returns it as a float.
    Returns NaN if not found.
    """
    x, y = _as_numpy_xy(signal_view)
    n = y.size
    if n < 2:
        return float("nan")

    if threshold is None:
        threshold = _auto_threshold_from_ends(y)

    thr_up = threshold + float(atol)
    thr_dn = threshold - float(atol)

    low_run = 0
    i = 0
    while i < n - 1:
        # Track persistence of the 'low' side
        if (y[i] <= thr_dn) if edge == "rising" else (y[i] >= thr_up):
            low_run += 1
        else:
            low_run = 0

        if low_run >= min_low_samples:
            crossed = (
                (y[i] <= thr_dn and y[i + 1] >= thr_up) if edge == "rising"
                else (y[i] >= thr_up and y[i + 1] <= thr_dn)
            )
            if crossed:
                j_start = i + 1
                j_end = min(n, j_start + min_high_samples)
                if j_start >= n:
                    return float("nan")
                stable_high = (
                    np.all(y[j_start:j_end] >= thr_up) if edge == "rising"
                    else np.all(y[j_start:j_end] <= thr_dn)
                )
                if stable_high:
                    return _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], float(threshold))
        i += 1

    return float("nan")

def find_burst_start(
    signal_view,
    threshold: Optional[float] = None,
    edge: Edge = "rising",
    min_quiet_gap: float = 0.01,
    require_edges_after: int = 3,
    window_after: float = 0.01,
    atol: float = 1e-8
) -> float:
    """
    Detects the start of a pulse train ("burst") and returns the first burst edge time as float.
    Returns NaN if not found.
    """
    x, y = _as_numpy_xy(signal_view)
    if threshold is None:
        threshold = _auto_threshold_from_ends(y)

    edges = _find_all_edge_times(signal_view, float(threshold), edge=edge, atol=atol)
    if edges.size == 0:
        return float("nan")

    if edges.size == 1:
        if require_edges_after <= 0:
            return float(edges[0])
        return float("nan")

    gaps = np.diff(edges)
    idxs = np.flatnonzero(gaps >= float(min_quiet_gap))
    start_idx = 0 if idxs.size == 0 else int(idxs[0] + 1)
    t0 = float(edges[start_idx])

    if require_edges_after <= 0:
        return t0

    t_end = t0 + float(window_after)
    count_after = int(np.sum((edges >= t0) & (edges <= t_end)))
    if count_after >= require_edges_after:
        return t0
    return float("nan")

def detect_change_start(
    signal_view,
    mode: Literal["step", "burst"] = "burst",
    threshold: Optional[float] = None,
    edge: Edge = "rising",
    atol: float = 1e-8,
    # Step-specific:
    min_low_samples: int = 3,
    min_high_samples: int = 3,
    # Burst-specific:
    min_quiet_gap: float = 0.01,
    require_edges_after: int = 3,
    window_after: float = 0.01,
) -> float:
    """
    Unified entry to detect when the signal's behavior 'starts changing'.
    Returns the timestamp as a float, or NaN if not found.
    """
    if mode == "step":
        return find_step_start(
            signal_view,
            threshold=threshold,
            edge=edge,
            min_low_samples=min_low_samples,
            min_high_samples=min_high_samples,
            atol=atol,
        )
    elif mode == "burst":
        return find_burst_start(
            signal_view,
            threshold=threshold,
            edge=edge,
            min_quiet_gap=min_quiet_gap,
            require_edges_after=require_edges_after,
            window_after=window_after,
            atol=atol,
        )
    else:
        raise ValueError("mode must be 'step' or 'burst'")

# ------------------------------ Misc utilities ------------------------------ #

def find_persistent_value(signal_view, value: float, atol: float = 1e-8) -> float:
    """
    Returns the timestamp (float) at which the signal becomes (and remains) at 'value'
    for all subsequent samples. Returns NaN if no such persistent value is found.
    """
    x, y = _as_numpy_xy(signal_view)
    for i in range(y.size):
        if np.all(np.isclose(y[i:], value, atol=atol)):
            return float(x[i])
    return float("nan")

def find_pattern(signal_view, pattern: List[float], atol: float = 1e-8) -> float:
    """
    Finds the first timestamp (float) where the signal matches a sequence of values in 'pattern'.
    Returns NaN if not found.
    """
    x, y = _as_numpy_xy(signal_view)
    n = len(pattern)
    if y.size < n:
        return float("nan")
    p = np.asarray(pattern, dtype=float)
    for i in range(y.size - n + 1):
        if np.all(np.isclose(y[i:i + n], p, atol=atol)):
            return float(x[i])
    return float("nan")

def evaluate_signal_trend(signal_view, sample_count: int = 1) -> List[str]:
    """
    Evaluates the signal's trend ('increasing', 'decreasing', 'unchanged')
    using every 'sample_count'-th interval.
    Returns a list of trend labels with length len(y) - sample_count.
    (No timestamp output here.)
    """
    _, y = _as_numpy_xy(signal_view)
    trends: List[str] = []
    N = y.size
    if sample_count < 1 or N <= sample_count:
        return trends
    for i in range(N - sample_count):
        diff = y[i + sample_count] - y[i]
        if np.isclose(diff, 0.0):
            trends.append("unchanged")
        elif diff > 0:
            trends.append("increasing")
        else:
            trends.append("decreasing")
    return trends

def find_signal_status(
    signal_view,
    check_type: Literal["stuck", "updating"] = "stuck",
    atol: float = 1e-8
) -> float:
    """
    Simplified to honor 'single float timestamp' rule.

    - If check_type == "stuck":
        Returns the timestamp (float) where the signal becomes stuck (constant thereafter),
        or NaN if it never becomes stuck.
    - If check_type == "updating":
        There is no single 'time' output; returns NaN always.
    """
    if check_type == "updating":
        return float("nan")

    x, y = _as_numpy_xy(signal_view)
    N = y.size
    for i in range(N):
        if np.all(np.isclose(y[i:], y[i], atol=atol)):
            return float(x[i])
    return float("nan")

# --------------------------- In-place scaling helper ------------------------ #

def scale_rec_signal(view, factor: float = 1.0, offset: float = 0.0, round_to: Optional[int] = None, clip: Optional[Tuple[float, float]] = None):
    """
    In-place scale when you pass ONLY the recorded view, e.g.:
      scale_rec_signal(rec["Device:Signal"], factor=2.0, offset=0.5)

    This updates the same object that rec["Device:Signal"] returns, so future
    accesses to rec["Device:Signal"].y reflect the new scaling.

    Parameters:
    - view: the _SignalView returned by rec["Device:Signal"]
    - factor: multiplicative factor
    - offset: additive offset
    - round_to: optional int, decimals to round to
    - clip: optional (min, max) tuple to clamp the result

    Returns:
    - The same view object with its data updated.
    """
    y = np.asarray(view.y, dtype=float) * float(factor) + float(offset)

    if round_to is not None:
        y = np.round(y, int(round_to))
    if clip is not None:
        lo, hi = clip
        y = np.clip(y, lo, hi)

    # Update underlying storage so future rec["Device:Signal"] uses the scaled data
    if hasattr(view, "_y"):
        view._y = y
    else:
        # Fallback: try to update via the public y array
        try:
            view.y[:] = y
        except Exception as exc:
            raise AttributeError(
                "Cannot update signal data in place for this RecorderResult implementation."
            ) from exc

    return view

















"""
Signal change detectors.

Includes:
- burst_time: Detects the start/end timestamp of a burst of dense threshold crossings.
- pattern_major_fault_time: Detects the first MAJOR disturbance time in a repeating pattern.

No user-tunable parameters are exposed beyond the required function arguments.
"""

from typing import Tuple, Literal
import numpy as np

Edge = Literal["rising", "falling"]

# ============================== Common utilities ==============================

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    """
    Return x, y as numpy arrays, drop NaNs in sync, and ensure time is strictly ascending.
    If there are duplicate timestamps, they are stably sorted and kept as-is.
    """
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)

    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")

    # Drop NaNs (synchronized)
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]

    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x, kind="stable")
        x, y = x[idx], y[idx]

    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Resample to a uniform grid using linear interpolation.
    Returns (tu, yu, dt). If dt is not finite or too small, returns (x, y, inf).
    """
    if x.size < 8:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 8:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    """
    Robust median and sigma via MAD (scaled). Falls back to std if needed.
    """
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad
    if not np.isfinite(sigma) or sigma == 0:
        s = float(np.std(a))
        sigma = s if s > 0 else 1e-12
    return med, sigma

def _interp_cross_time(x0: float, x1: float, y0: float, y1: float, thr: float) -> float:
    """Linear interpolation for threshold crossing at level thr between (x0,y0) and (x1,y1)."""
    if y1 == y0:
        return float(x0)
    return float(x0 + (x1 - x0) * (thr - y0) / (y1 - y0))


# ============================== Burst detector ===============================

def _schmitt_edges(signal_view, threshold: float, atol: float, edge: Edge) -> np.ndarray:
    """
    Schmitt-trigger style edge extractor with hysteresis band [thr-atol, thr+atol].
    - For 'rising', records times when the state flips from BELOW to ABOVE.
    - For 'falling', records times when the state flips from ABOVE to BELOW.
    Uses linear interpolation to timestamp the crossing at 'threshold'.
    """
    x, y = _as_numpy_xy(signal_view)
    n = x.size
    if n < 2:
        return np.array([], dtype=float)

    thr = float(threshold)
    up = thr + float(atol)
    dn = thr - float(atol)

    # Initial state
    above = y[0] > up
    below = y[0] < dn
    state = 1 if above else (0 if below else None)

    times = []
    for i in range(n - 1):
        y0, y1 = float(y[i]), float(y[i + 1])
        x0, x1 = float(x[i]), float(x[i + 1])

        # Update state only after crossing the opposite bound
        if state is None:
            # Inside deadband: decide if we left it
            if y1 >= up:
                state = 1
                if edge == "rising":
                    times.append(_interp_cross_time(x0, x1, y0, y1, thr))
            elif y1 <= dn:
                state = 0
                if edge == "falling":
                    times.append(_interp_cross_time(x0, x1, y0, y1, thr))
            continue

        if state == 0:
            # Currently below; look for rising transition only if y crosses 'up'
            if y0 <= dn and y1 >= up:
                state = 1
                if edge == "rising":
                    times.append(_interp_cross_time(x0, x1, y0, y1, thr))
            elif y0 <= dn and dn < y1 < up:
                # Enter deadband upward: wait until leaving at 'up'
                if y1 >= up:
                    state = 1
                    if edge == "rising":
                        times.append(_interp_cross_time(x0, x1, y0, y1, thr))
        else:
            # state == 1: currently above; look for falling transition only if y crosses 'dn'
            if y0 >= up and y1 <= dn:
                state = 0
                if edge == "falling":
                    times.append(_interp_cross_time(x0, x1, y0, y1, thr))
            elif y0 >= up and dn < y1 < up:
                # Enter deadband downward: wait until leaving at 'dn'
                if y1 <= dn:
                    state = 0
                    if edge == "falling":
                        times.append(_interp_cross_time(x0, x1, y0, y1, thr))

    if not times:
        return np.array([], dtype=float)
    return np.array(times, dtype=float)

def _dense_runs_from_edges(edges: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Identify dense toggling runs from edge times using an adaptive gap threshold.
    Returns (run_starts, run_ends, gap_threshold) where starts/ends are indices into gaps.
    A run of gaps[i0..i1] corresponds to edges[i0]..edges[i1+1].
    """
    if edges.size < 4:
        return np.array([], dtype=int), np.array([], dtype=int), float("nan")

    gaps = np.diff(edges)
    if gaps.size == 0:
        return np.array([], dtype=int), np.array([], dtype=int), float("nan")

    med_all = float(np.median(gaps))
    p25_all = float(np.percentile(gaps, 25))
    # Use early portion as baseline if available
    k_learn = max(5, int(0.3 * gaps.size))
    med_base = float(np.median(gaps[:k_learn])) if gaps.size >= k_learn else med_all

    # Adaptive threshold favors dense (short) gaps; conservative relative to baseline
    thr = max(1e-12, min(0.6 * med_all, 1.2 * p25_all, 0.8 * med_base))

    small = gaps <= thr
    if not np.any(small):
        return np.array([], dtype=int), np.array([], dtype=int), thr

    # Find contiguous True segments in 'small'
    b = small.astype(np.int8)
    db = np.diff(b)
    # Starts are where we go 0->1; also include if first is True
    starts = np.flatnonzero(np.concatenate(([b[0] == 1], db == 1)))
    # Ends are where we go 1->0; also include if last is True
    ends = np.flatnonzero(np.concatenate((db == -1, [b[-1] == 1])))

    # Keep only runs with at least 3 small gaps (>= 4 edges in the run)
    valid = (ends - starts + 1) >= 3
    return starts[valid], ends[valid], thr

def burst_time(
    signal_view,
    value: float,
    atol: float,
    edge: Edge,
    burst: bool = True,  # kept for signature clarity; always treated as burst detection
) -> float:
    """
    Single-timestamp burst detector.

    - edge='rising'  -> returns the burst START time (first edge of dense run)
    - edge='falling' -> returns the burst END time   (last edge of dense run)

    Returns float('nan') if not detected.
    """
    edges = _schmitt_edges(signal_view, value, atol, edge)
    if edges.size < 4:
        return float("nan")

    run_starts, run_ends, _thr = _dense_runs_from_edges(edges)
    if run_starts.size == 0:
        return float("nan")

    if edge == "rising":
        # Choose earliest dense run (start of burst)
        i0 = int(run_starts[0])
        return float(edges[i0])
    else:
        # Choose latest dense run (end of burst)
        i1 = int(run_ends[-1]) + 1  # convert gap-index to edge index
        i1 = min(i1, edges.size - 1)
        return float(edges[i1])


# ===================== Major repeating pattern change detector =====================

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via FFT of zero-mean data; NaN if not reliable."""
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0  # ignore DC
    fmin = 3.0 / (N * dt)   # at least ~3 cycles in learn window
    fmax = 0.45 / dt        # <= 90% Nyquist
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    idx = np.flatnonzero(band)[np.argmax(mag[band])]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via autocorrelation peak; NaN if not reliable."""
    N = y.size
    if N < 128 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    L = int(1 << (N - 1).bit_length())
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

def _align_and_average(y: np.ndarray, spp: int, max_periods: int = 12, template_points: int = 128) -> np.ndarray:
    """
    Build a normalized template by aligning up to max_periods cycles (length spp),
    using circular shift to maximize correlation with the first cycle.
    """
    n = y.size
    spp = int(max(8, spp))
    K = min(max_periods, n // spp)
    if K < 2:
        return np.array([], dtype=float)

    ref = y[0:spp].astype(float)
    ref = ref - np.mean(ref)
    s = float(np.std(ref))
    if s > 0:
        ref /= s

    acc = np.zeros(spp, dtype=float)
    cnt = 0
    for i in range(K):
        seg = y[i * spp:(i + 1) * spp].astype(float)
        if seg.size != spp:
            break
        seg = seg - np.mean(seg)
        std = float(np.std(seg))
        if std > 0:
            seg = seg / std
        c = np.fft.ifft(np.fft.fft(ref) * np.conj(np.fft.fft(seg))).real
        shift = int(np.argmax(c)) % spp
        acc += np.roll(seg, shift)
        cnt += 1

    if cnt == 0:
        return np.array([], dtype=float)

    avg = acc / cnt
    xi = np.linspace(0, 1, spp, endpoint=False)
    phase_grid = np.linspace(0, 1, template_points, endpoint=False)
    tpl = np.interp(phase_grid, xi, avg)
    norm = float(np.linalg.norm(tpl))
    if norm > 0:
        tpl = tpl / norm
    return tpl

def _window_features(y: np.ndarray, i: int, spp: int, template: np.ndarray) -> Tuple[float, float, float, float]:
    """
    Features for window y[i:i+spp]:
    - correlation to template (normalized)
    - mean
    - std
    - peak-to-peak amplitude
    """
    seg = y[i:i + spp]
    if seg.size != spp:
        return np.nan, np.nan, np.nan, np.nan
    m = float(np.mean(seg))
    s = float(np.std(seg))
    a = float(np.max(seg) - np.min(seg))
    # normalized correlation
    z = seg - m
    if s > 0:
        z = z / s
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    seg_t = np.interp(np.linspace(0, 1, template.size, endpoint=False), xi, z)
    denom = float(np.linalg.norm(seg_t))
    c = float(np.dot(seg_t, template) / denom) if denom > 0 else 0.0
    return c, m, s, a

def _period_diff_series(yu: np.ndarray, tu: np.ndarray, spp: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    Sample-level period-difference detector: d[t] = |y[t] - y[t - spp]|.
    Returns (td, d); td aligns with yu[spp:].
    """
    if spp < 2 or yu.size <= spp:
        return np.array([], dtype=float), np.array([], dtype=float)
    d = np.abs(yu[spp:] - yu[:-spp])
    td = tu[spp:]
    # light median smoothing (k=5) to reduce spikiness
    if d.size >= 5:
        k = 5
        pad = k // 2
        dp = np.pad(d, (pad, pad), mode="edge")
        d = np.median(np.stack([dp[i:i + d.size] for i in range(k)], axis=0), axis=0)
    return td, d

def _refine_time_with_period_diff(
    tu: np.ndarray,
    yu: np.ndarray,
    spp: int,
    learn_end_time: float,
    t_init: float,
    md: float,
    sd: float,
    dt: float,
) -> float:
    """
    Refine coarse trigger time using the first sustained crossing on d[t] near t_init.
    Returns NaN if refinement cannot confirm a nearby onset.
    """
    td, d = _period_diff_series(yu, tu, spp)
    if td.size == 0 or not np.isfinite(sd) or sd == 0:
        return float("nan")

    T = spp * dt
    t_lo = max(learn_end_time, t_init - 0.25 * T)
    t_hi = min(tu[-1], t_init + 1.25 * T)
    m = (td >= t_lo) & (td <= t_hi)
    if not np.any(m):
        return float("nan")

    thr = md + 4.0 * sd  # strict above-baseline threshold
    hold = max(3, int(round(0.10 * spp)))  # ~10% of a period

    run = 0
    for j in np.flatnonzero(m):
        if d[j] > thr:
            run += 1
            if run >= hold:
                t_ref = float(td[j - hold + 1])
                t_ref = max(t_ref, learn_end_time)
                t_ref = min(t_ref, t_init + T)
                return t_ref
        else:
            run = 0
    return float("nan")

def pattern_major_fault_time(signal_view) -> float:
    """
    Return the first timestamp (float seconds) where a MAJOR disturbance occurs.
    Minor changes are ignored by design. Returns float('nan') if none detected.

    Strategy:
    - Learn template from first 30% (max half).
    - Slide a window of one period with hop ~T/12; use center-of-window timestamps.
    - Detect:
      A) Large correlation drop (c < min(mc - 4Ïƒ, 0.55)), sustained ~35% of a period,
         then refine onset with sample-level period-diff and require confirmation.
      B) CUSUM-like via sigma test on mean/std/amp: require at least two z-scores > 6,
         sustained ~35% of a period.
    - Return the earliest confirmed time, clamped to not earlier than learn_end_time.
    """
    x, y = _as_numpy_xy(signal_view)
    if x.size < 256:
        return float("nan")

    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 256:
        return float("nan")

    # Learn region
    learn_fraction = 0.30
    N = tu.size
    N_learn = min(max(256, int(N * learn_fraction)), max(256, N // 2))
    yL = yu[:N_learn]
    learn_end_time = tu[min(N_learn - 1, N - 1)]

    # Period estimate
    T = _best_period(yL, dt)
    if not np.isfinite(T):
        return float("nan")
    spp = int(round(T / dt))
    if spp < 12 or spp > N_learn // 2:
        return float("nan")

    # Template
    template = _align_and_average(yL, spp, max_periods=12, template_points=128)
    if template.size == 0:
        return float("nan")

    # Sliding features
    hop = max(1, int(round(spp / 12)))  # ~12 hops per period for better time resolution
    times = []
    corr = []
    meanv = []
    stdv = []
    ampv = []
    for i in range(0, N - spp, hop):
        c, m, s, a = _window_features(yu, i, spp, template)
        times.append(tu[i + spp // 2])  # center-of-window to avoid early bias
        corr.append(c)
        meanv.append(m)
        stdv.append(s)
        ampv.append(a)

    times = np.asarray(times, dtype=float)
    corr = np.asarray(corr, dtype=float)
    meanv = np.asarray(meanv, dtype=float)
    stdv = np.asarray(stdv, dtype=float)
    ampv = np.asarray(ampv, dtype=float)
    if times.size == 0:
        return float("nan")

    learn_mask = times <= learn_end_time
    if not np.any(learn_mask):
        return float("nan")

    mc, sc = _robust_mean_std(corr[learn_mask])
    mm, sm = _robust_mean_std(meanv[learn_mask])
    ms, ss = _robust_mean_std(stdv[learn_mask])
    ma, sa = _robust_mean_std(ampv[learn_mask])
    if not all(np.isfinite(v) for v in (mc, sc, mm, sm, ms, ss, ma, sa)):
        return float("nan")

    # Detector A: correlation drop (shape change), sustained, with interpolation and refinement
    c_thr = min(mc - 4.0 * sc, 0.55)  # conservative absolute cap
    hold_windows = max(3, int(round(0.35 * spp / hop)))  # ~35% of a period sustained
    j0 = int(np.searchsorted(times, learn_end_time, side="right"))
    bad = corr < c_thr

    # Precompute period-diff baseline for refinement
    td_all, d_all = _period_diff_series(yu, tu, spp)
    md = sd = float("nan")
    if td_all.size > 0:
        m_d_learn = td_all <= learn_end_time
        if np.any(m_d_learn):
            md, sd = _robust_mean_std(d_all[m_d_learn])

    tA = float("nan")
    run = 0
    for j in range(j0, times.size):
        if bad[j]:
            run += 1
            if run >= hold_windows:
                j_start = j - hold_windows + 1
                k = j_start - 1
                # Linear interpolate the threshold crossing between last good and first bad
                if k >= j0 and np.isfinite(corr[k]):
                    y1, t1 = corr[k], times[k]
                    y2, t2 = corr[j_start], times[j_start]
                    if y2 != y1:
                        alpha = (c_thr - y1) / (y2 - y1)
                        alpha = float(np.clip(alpha, 0.0, 1.0))
                        t_cross = t1 + alpha * (t2 - t1)
                    else:
                        t_cross = times[j_start]
                else:
                    t_cross = times[j_start]

                # Confirm and refine with period-diff
                t_ref = float("nan")
                if np.isfinite(md) and np.isfinite(sd) and sd > 0:
                    t_ref = _refine_time_with_period_diff(tu, yu, spp, learn_end_time, t_cross, md, sd, dt)
                if np.isfinite(t_ref):
                    tA = t_ref
                    break
                else:
                    # If not confirmed, continue scanning
                    run = 0
        else:
            run = 0

    # Detector B: feature sigma test (mean/std/amp), sustained
    z_mean = np.abs(meanv - mm) / max(sm, 1e-12)
    z_std  = np.abs(stdv - ms)  / max(ss, 1e-12)
    z_amp  = np.abs(ampv - ma)  / max(sa, 1e-12)
    major_feat = ((z_mean > 6.0).astype(int) + (z_std > 6.0).astype(int) + (z_amp > 6.0).astype(int)) >= 2

    tB = float("nan")
    run = 0
    for j in range(j0, times.size):
        if major_feat[j]:
            run += 1
            if run >= hold_windows:
                tB = float(times[j - hold_windows + 1])
                break
        else:
            run = 0

    # Final selection (earliest confirmed)
    candidates = [t for t in (tA, tB) if np.isfinite(t)]
    if not candidates:
        return float("nan")
    return float(max(min(candidates), learn_end_time))


__all__ = [
    "burst_time",
    "pattern_major_fault_time",
]
