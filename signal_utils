"""
Signal analysis helpers updated to return a single float timestamp for any API that outputs a time.

Conventions for time-returning functions:
- Always return a Python float (seconds).
- If an event is not found, return float("nan") instead of None or arrays.

Provided features:
- Robust edge detection (first edge only as public API).
- Step start detection (stable low -> crossing -> stable high).
- Burst start detection (first edge after a quiet gap with optional confirmation).
- Interpolation helpers (value at time, time at value).
- Misc utilities (persistent value time, pattern match time, trends, stuck start time).
- In-place scaling helper for recorder results (_SignalView-like objects).

Expected 'signal_view' interface:
- signal_view.x: 1D array-like of timestamps (preferably increasing).
- signal_view.y: 1D array-like of values (same length as x).

Author: GitHub Copilot (updated per "single float timestamp" requirement)
"""

from typing import List, Literal, Tuple, Union, Optional
import numpy as np

Edge = Literal["rising", "falling"]

# ---------------------------- Internal utilities ---------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    """Return x, y as numpy arrays and ensure they are sorted by time ascending."""
    # Accept either a (x, y) tuple or an object with .x and .y
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(signal_view.x, dtype=float)
        y = np.asarray(signal_view.y, dtype=float)

    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")

    # Ensure increasing order in time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x = x[idx]
        y = y[idx]
    return x, y

def _interp_cross_time(x0: float, x1: float, y0: float, y1: float, thr: float) -> float:
    """Linear interpolation to find the crossing time at threshold thr between two points."""
    if y1 == y0:
        return float(x0)
    return float(x0 + (x1 - x0) * (thr - y0) / (y1 - y0))

# ------------------------- Interpolation convenience ------------------------ #

def gettimestampbyvalue(signal_view, value: float) -> float:
    """
    Returns the first timestamp where the signal crosses a given 'value' as a float.
    - If the value is exactly present, returns its first occurrence.
    - Otherwise, uses linear interpolation between the two points around the crossing.
    - Returns NaN if no crossing found or value is outside the signal's range.
    """
    x, y = _as_numpy_xy(signal_view)
    if y.size < 2 or value < np.min(y) or value > np.max(y):
        return float("nan")

    # exact match first
    exact = np.flatnonzero(y == value)
    if exact.size > 0:
        return float(x[int(exact[0])])

    # sign change around value
    indices = np.flatnonzero(np.diff(np.sign(y - value)) != 0)
    if indices.size == 0:
        return float("nan")

    i = int(indices[0])
    return _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], value)

def getvaluebytimestamp(signal_view, timestamp: float) -> Optional[float]:
    """
    Returns the signal value at a given 'timestamp' (linear interpolation).
    - If the timestamp is exactly present, returns the exact sample value.
    - Otherwise, linearly interpolates using the two neighboring timestamps.
    - Returns None if timestamp is outside [min(x), max(x)] or x has fewer than 2 samples.
    Note: This function returns a value (not a time), so it keeps Optional[float].
    """
    x, y = _as_numpy_xy(signal_view)
    if x.size < 2 or timestamp < x[0] or timestamp > x[-1]:
        return None

    # Exact match?
    exact = np.flatnonzero(x == timestamp)
    if exact.size > 0:
        return float(y[int(exact[0])])

    # Locate insertion point
    j = np.searchsorted(x, timestamp)
    if j == 0 or j == x.size:
        return None  # out of bounds safeguard
    i = j - 1
    x0, x1 = x[i], x[i + 1]
    y0, y1 = y[i], y[i + 1]
    if x1 == x0:
        return float(y0)
    v = y0 + (y1 - y0) * (timestamp - x0) / (x1 - x0)
    return float(v)

# ------------------------------- Edge finding ------------------------------- #

def _interp_cross_time(x0: float, x1: float, y0: float, y1: float, thr: float) -> float:
    # Linear interpolation for the crossing at threshold thr
    if y1 == y0:
        return float(x0)
    return float(x0 + (x1 - x0) * (thr - y0) / (y1 - y0))

def find_edge_time(signal_view, value: float, atol: float, edge: Edge) -> float:
    """
    First edge crossing time at 'value' with direction 'edge'.
    Returns float('nan') if not found.
    """
    x, y = _as_numpy_xy(signal_view)
    n = y.size
    if n < 2:
        return float("nan")

    thr = float(value)
    tol = float(atol)
    thr_up = thr + tol
    thr_dn = thr - tol

    # A) Hysteresis crossing
    if edge == "rising":
        mask = (y[:-1] <= thr_dn) & (y[1:] >= thr_up)
    elif edge == "falling":
        mask = (y[:-1] >= thr_up) & (y[1:] <= thr_dn)
    else:
        raise ValueError("edge must be 'rising' or 'falling'")

    idxs = np.flatnonzero(mask)
    if idxs.size > 0:
        i = int(idxs[0])
        return _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], thr)

    # B) Exact-sample match within tolerance
    exact = np.flatnonzero(np.abs(y - thr) <= tol)
    if exact.size > 0:
        return float(x[int(exact[0])])

    # C) Non-hysteresis crossing (touching counts)
    if edge == "rising":
        mask2 = (y[:-1] <= thr) & (y[1:] >= thr)
    else:  # falling
        mask2 = (y[:-1] >= thr) & (y[1:] <= thr)

    idxs2 = np.flatnonzero(mask2)
    if idxs2.size > 0:
        i = int(idxs2[0])
        return _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], thr)

    return float("nan")

def time_between_signals(
    signal_view1,
    signal_view2,
    value1: float,
    value2: float,
    edge1: Edge = "rising",
    edge2: Edge = "rising",
    atol1: float = 1e-8,
    atol2: float = 1e-8
) -> float:
    """
    Returns the absolute time difference between the first edge crossings
    in signal_view1 and signal_view2 at the specified thresholds and edge directions.
    Returns NaN if either crossing is not found.
    """
    t1 = find_first_edge_time(signal_view1, value1, edge=edge1, atol=atol1)
    t2 = find_first_edge_time(signal_view2, value2, edge=edge2, atol=atol2)
    if np.isnan(t1) or np.isnan(t2):
        return float("nan")
    return float(abs(t2 - t1))

# ------------------------ Change-start specialized finders ------------------- #

def _auto_threshold_from_ends(y: np.ndarray) -> float:
    """Estimate a mid-level threshold from the first and last 10% (or at least 5 samples) of y."""
    n = y.size
    if n == 0:
        return 0.0
    w = max(5, n // 10)
    low_med = float(np.median(y[:w]))
    high_med = float(np.median(y[-w:]))
    return 0.5 * (low_med + high_med)

def find_step_start(
    signal_view,
    threshold: Optional[float] = None,
    edge: Edge = "rising",
    min_low_samples: int = 3,
    min_high_samples: int = 3,
    atol: float = 1e-8
) -> float:
    """
    Detects the start time of a stable step change and returns it as a float.
    Returns NaN if not found.
    """
    x, y = _as_numpy_xy(signal_view)
    n = y.size
    if n < 2:
        return float("nan")

    if threshold is None:
        threshold = _auto_threshold_from_ends(y)

    thr_up = threshold + float(atol)
    thr_dn = threshold - float(atol)

    low_run = 0
    i = 0
    while i < n - 1:
        # Track persistence of the 'low' side
        if (y[i] <= thr_dn) if edge == "rising" else (y[i] >= thr_up):
            low_run += 1
        else:
            low_run = 0

        if low_run >= min_low_samples:
            crossed = (
                (y[i] <= thr_dn and y[i + 1] >= thr_up) if edge == "rising"
                else (y[i] >= thr_up and y[i + 1] <= thr_dn)
            )
            if crossed:
                j_start = i + 1
                j_end = min(n, j_start + min_high_samples)
                if j_start >= n:
                    return float("nan")
                stable_high = (
                    np.all(y[j_start:j_end] >= thr_up) if edge == "rising"
                    else np.all(y[j_start:j_end] <= thr_dn)
                )
                if stable_high:
                    return _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], float(threshold))
        i += 1

    return float("nan")

def find_burst_start(
    signal_view,
    threshold: Optional[float] = None,
    edge: Edge = "rising",
    min_quiet_gap: float = 0.01,
    require_edges_after: int = 3,
    window_after: float = 0.01,
    atol: float = 1e-8
) -> float:
    """
    Detects the start of a pulse train ("burst") and returns the first burst edge time as float.
    Returns NaN if not found.
    """
    x, y = _as_numpy_xy(signal_view)
    if threshold is None:
        threshold = _auto_threshold_from_ends(y)

    edges = _find_all_edge_times(signal_view, float(threshold), edge=edge, atol=atol)
    if edges.size == 0:
        return float("nan")

    if edges.size == 1:
        if require_edges_after <= 0:
            return float(edges[0])
        return float("nan")

    gaps = np.diff(edges)
    idxs = np.flatnonzero(gaps >= float(min_quiet_gap))
    start_idx = 0 if idxs.size == 0 else int(idxs[0] + 1)
    t0 = float(edges[start_idx])

    if require_edges_after <= 0:
        return t0

    t_end = t0 + float(window_after)
    count_after = int(np.sum((edges >= t0) & (edges <= t_end)))
    if count_after >= require_edges_after:
        return t0
    return float("nan")

def detect_change_start(
    signal_view,
    mode: Literal["step", "burst"] = "burst",
    threshold: Optional[float] = None,
    edge: Edge = "rising",
    atol: float = 1e-8,
    # Step-specific:
    min_low_samples: int = 3,
    min_high_samples: int = 3,
    # Burst-specific:
    min_quiet_gap: float = 0.01,
    require_edges_after: int = 3,
    window_after: float = 0.01,
) -> float:
    """
    Unified entry to detect when the signal's behavior 'starts changing'.
    Returns the timestamp as a float, or NaN if not found.
    """
    if mode == "step":
        return find_step_start(
            signal_view,
            threshold=threshold,
            edge=edge,
            min_low_samples=min_low_samples,
            min_high_samples=min_high_samples,
            atol=atol,
        )
    elif mode == "burst":
        return find_burst_start(
            signal_view,
            threshold=threshold,
            edge=edge,
            min_quiet_gap=min_quiet_gap,
            require_edges_after=require_edges_after,
            window_after=window_after,
            atol=atol,
        )
    else:
        raise ValueError("mode must be 'step' or 'burst'")

# ------------------------------ Misc utilities ------------------------------ #

def find_persistent_value(signal_view, value: float, atol: float = 1e-8) -> float:
    """
    Returns the timestamp (float) at which the signal becomes (and remains) at 'value'
    for all subsequent samples. Returns NaN if no such persistent value is found.
    """
    x, y = _as_numpy_xy(signal_view)
    for i in range(y.size):
        if np.all(np.isclose(y[i:], value, atol=atol)):
            return float(x[i])
    return float("nan")

def find_pattern(signal_view, pattern: List[float], atol: float = 1e-8) -> float:
    """
    Finds the first timestamp (float) where the signal matches a sequence of values in 'pattern'.
    Returns NaN if not found.
    """
    x, y = _as_numpy_xy(signal_view)
    n = len(pattern)
    if y.size < n:
        return float("nan")
    p = np.asarray(pattern, dtype=float)
    for i in range(y.size - n + 1):
        if np.all(np.isclose(y[i:i + n], p, atol=atol)):
            return float(x[i])
    return float("nan")

def evaluate_signal_trend(signal_view, sample_count: int = 1) -> List[str]:
    """
    Evaluates the signal's trend ('increasing', 'decreasing', 'unchanged')
    using every 'sample_count'-th interval.
    Returns a list of trend labels with length len(y) - sample_count.
    (No timestamp output here.)
    """
    _, y = _as_numpy_xy(signal_view)
    trends: List[str] = []
    N = y.size
    if sample_count < 1 or N <= sample_count:
        return trends
    for i in range(N - sample_count):
        diff = y[i + sample_count] - y[i]
        if np.isclose(diff, 0.0):
            trends.append("unchanged")
        elif diff > 0:
            trends.append("increasing")
        else:
            trends.append("decreasing")
    return trends

def find_signal_status(
    signal_view,
    check_type: Literal["stuck", "updating"] = "stuck",
    atol: float = 1e-8
) -> float:
    """
    Simplified to honor 'single float timestamp' rule.

    - If check_type == "stuck":
        Returns the timestamp (float) where the signal becomes stuck (constant thereafter),
        or NaN if it never becomes stuck.
    - If check_type == "updating":
        There is no single 'time' output; returns NaN always.
    """
    if check_type == "updating":
        return float("nan")

    x, y = _as_numpy_xy(signal_view)
    N = y.size
    for i in range(N):
        if np.all(np.isclose(y[i:], y[i], atol=atol)):
            return float(x[i])
    return float("nan")

# --------------------------- In-place scaling helper ------------------------ #

def scale_rec_signal(view, factor: float = 1.0, offset: float = 0.0, round_to: Optional[int] = None, clip: Optional[Tuple[float, float]] = None):
    """
    In-place scale when you pass ONLY the recorded view, e.g.:
      scale_rec_signal(rec["Device:Signal"], factor=2.0, offset=0.5)

    This updates the same object that rec["Device:Signal"] returns, so future
    accesses to rec["Device:Signal"].y reflect the new scaling.

    Parameters:
    - view: the _SignalView returned by rec["Device:Signal"]
    - factor: multiplicative factor
    - offset: additive offset
    - round_to: optional int, decimals to round to
    - clip: optional (min, max) tuple to clamp the result

    Returns:
    - The same view object with its data updated.
    """
    y = np.asarray(view.y, dtype=float) * float(factor) + float(offset)

    if round_to is not None:
        y = np.round(y, int(round_to))
    if clip is not None:
        lo, hi = clip
        y = np.clip(y, lo, hi)

    # Update underlying storage so future rec["Device:Signal"] uses the scaled data
    if hasattr(view, "_y"):
        view._y = y
    else:
        # Fallback: try to update via the public y array
        try:
            view.y[:] = y
        except Exception as exc:
            raise AttributeError(
                "Cannot update signal data in place for this RecorderResult implementation."
            ) from exc

    return view


















"""
Burst change timestamp detector (single float output).

You pass ONLY:
- signal_view: object with .x (timestamps) and .y (values), or a tuple (x, y)
- value: float threshold
- atol: float hysteresis tolerance
- edge: "rising" or "falling"
- burst: bool (kept for signature clarity; always treated as burst detection)

Behavior
- edge == "rising": return the start time of the burst (first edge where dense toggling begins)
- edge == "falling": return the end time of the burst (last edge where dense toggling ends)

Returns
- float timestamp in seconds
- float("nan") if not detected

Notes
- No extra tuning parameters are accepted; adaptive detection is used internally.
"""

from typing import Tuple, Literal
import numpy as np

Edge = Literal["rising", "falling"]

# ------------------------------ Small helpers ------------------------------ #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    """Return x, y as numpy arrays and ensure time is ascending."""
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)
    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]
    return x, y

def _interp_cross_time(x0: float, x1: float, y0: float, y1: float, thr: float) -> float:
    """Linear interpolation for threshold crossing."""
    if y1 == y0:
        return float(x0)
    return float(x0 + (x1 - x0) * (thr - y0) / (y1 - y0))

def _find_all_edge_times(signal_view, threshold: float, edge: Edge, atol: float) -> np.ndarray:
    """All crossing times using hysteresis band [thr-atol, thr+atol]."""
    x, y = _as_numpy_xy(signal_view)
    if x.size < 2:
        return np.array([], dtype=float)
    thr_up = float(threshold) + float(atol)
    thr_dn = float(threshold) - float(atol)
    if edge == "rising":
        mask = (y[:-1] <= thr_dn) & (y[1:] >= thr_up)
    else:
        mask = (y[:-1] >= thr_up) & (y[1:] <= thr_dn)
    idxs = np.flatnonzero(mask)
    if idxs.size == 0:
        return np.array([], dtype=float)
    times = np.empty(idxs.size, dtype=float)
    for k, i in enumerate(idxs):
        times[k] = _interp_cross_time(x[i], x[i + 1], y[i], y[i + 1], float(threshold))
    return times

def _dense_runs_from_edges(edges: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Identify dense toggling runs from edge times using an adaptive gap threshold.
    Returns (run_starts, run_ends, gap_threshold).
    run_starts/ends are indices into the 'gaps' array (length edges.size-1).
    A run of gaps[i0..i1] corresponds to edges[i0]..edges[i1+1].
    """
    if edges.size < 3:
        return np.array([], dtype=int), np.array([], dtype=int), float("nan")
    gaps = np.diff(edges)
    if gaps.size == 0:
        return np.array([], dtype=int), np.array([], dtype=int), float("nan")

    med = float(np.median(gaps))
    p20 = float(np.percentile(gaps, 20))
    # Adaptive threshold: favor smaller (denser) gap criterion.
    thr = max(1e-12, min(med * 0.7, p20 * 1.5))

    small = gaps <= thr
    if not np.any(small):
        return np.array([], dtype=int), np.array([], dtype=int), thr

    # Find contiguous True runs
    # Convert boolean runs to start/end indices on 'gaps'
    diffs = np.diff(small.astype(int))
    starts = np.flatnonzero(np.concatenate(([small[0]], diffs == 1)))
    ends = np.flatnonzero(np.concatenate(((diffs == -1), [small[-1]])))  # inclusive ends on gaps index
    # Require at least 2 small gaps in a run (=> at least 3 edges in that dense segment)
    valid = (ends - starts + 1) >= 2
    return starts[valid], ends[valid], thr

# --------------------------------- Public ---------------------------------- #

def burst_time(
    signal_view,
    value: float,
    atol: float,
    edge: Edge,
    burst: bool = True,  # kept for signature clarity; always treated as burst detection
) -> float:
    """
    Single-timestamp burst detector.

    - edge='rising'  -> returns the burst START time (first edge of dense run)
    - edge='falling' -> returns the burst END time   (last edge of dense run)

    Returns float('nan') if not detected.
    """
    edges = _find_all_edge_times(signal_view, value, edge, atol)
    if edges.size < 3:
        return float("nan")

    run_starts, run_ends, _thr = _dense_runs_from_edges(edges)
    if run_starts.size == 0:
        return float("nan")

    if edge == "rising":
        # First dense run start -> edges[run_start]
        i0 = int(run_starts[0])
        return float(edges[i0])
    else:
        # Last dense run end -> edges[run_end + 1]
        i1 = int(run_ends[-1])
        j = i1 + 1
        if j < edges.size:
            return float(edges[j])
        # Fallback safety (shouldn't happen): use last edge
        return float(edges[-1])

__all__ = ["burst_time"]

# ------------------------------- Public API -------------------------------- #

__all__ = [
    # Interpolation
    "gettimestampbyvalue",      # float (NaN if not found)
    "getvaluebytimestamp",      # Optional[float] (returns a value, not a time)
    # Edges (single timestamp)
    "find_first_edge_time",     # float (NaN if none)
    "find_edge_times",          # float (alias of first edge)
    "time_between_signals",     # float (NaN if either missing)
    # Change start (single timestamp)
    "find_step_start",          # float (NaN if none)
    "find_burst_start",         # float (NaN if none)
    "detect_change_start",      # float (NaN if none)
    # Misc with time outputs simplified to float
    "find_persistent_value",    # float (NaN if none)
    "find_pattern",             # float (NaN if none)
    "evaluate_signal_trend",    # List[str] (no time)
    "find_signal_status",       # float (timestamp for 'stuck', NaN for 'updating')
    # Scaling
    "scale_rec_signal",
]




"""
Robustly detect when a repeating signal pattern becomes disturbed (fault start).

You pass ONLY:
- signal_view: object with .x (timestamps) and .y (values), or a tuple (x, y)

Returns:
- float timestamp (seconds) of the first disturbance, or float("nan") if not detected.

Key points
- No pattern is provided by the user: the function auto-learns it from the first 30% of data.
- Tolerant to small variations: uses robust statistics (median/MAD) to set adaptive thresholds.
- Multiple fallbacks: FFT and autocorrelation for period; correlation-based detection and a
  mean-level fallback to catch large DC/shape changes that still correlate moderately.

This is a drop-in replacement for earlier pattern_fault_time implementations that were too strict.
"""

from typing import Tuple
import numpy as np

# ------------------------------ Utilities ---------------------------------- #

def _as_numpy_xy(signal_view) -> Tuple[np.ndarray, np.ndarray]:
    # Accept (x, y) tuple or object with .x/.y
    if isinstance(signal_view, tuple) and len(signal_view) == 2:
        x = np.asarray(signal_view[0], dtype=float)
        y = np.asarray(signal_view[1], dtype=float)
    else:
        x = np.asarray(getattr(signal_view, "x"), dtype=float)
        y = np.asarray(getattr(signal_view, "y"), dtype=float)

    if x.ndim != 1 or y.ndim != 1 or x.size != y.size:
        raise ValueError("signal_view.x and signal_view.y must be 1D arrays of the same length.")

    # Drop NaNs (synchronized)
    if np.isnan(x).any() or np.isnan(y).any():
        m = (~np.isnan(x)) & (~np.isnan(y))
        x, y = x[m], y[m]

    # Ensure ascending time
    if x.size > 1 and not np.all(np.diff(x) >= 0):
        idx = np.argsort(x)
        x, y = x[idx], y[idx]

    return x, y

def _uniform_resample(x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:
    """Resample to a uniform grid using linear interpolation. Returns (tu, yu, dt)."""
    if x.size < 4:
        return x, y, np.inf
    dt = float(np.median(np.diff(x)))
    if not np.isfinite(dt) or dt <= 0:
        return x, y, np.inf
    tu = np.arange(x[0], x[-1] + 0.5 * dt, dt)
    if tu.size < 4:
        return x, y, np.inf
    yu = np.interp(tu, x, y)
    return tu, yu, dt

def _robust_mean_std(a: np.ndarray) -> Tuple[float, float]:
    """Median and robust sigma estimate via MAD (scaled)."""
    a = a[np.isfinite(a)]
    if a.size == 0:
        return float("nan"), float("nan")
    med = float(np.median(a))
    mad = float(np.median(np.abs(a - med)))
    sigma = 1.4826 * mad  # Gaussian consistent
    return med, sigma

# ---------------------------- Period estimation ----------------------------- #

def _estimate_period_fft(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via FFT of zero-mean data; NaN if not reliable."""
    N = y.size
    if N < 32 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    Z = np.fft.rfft(z)
    f = np.fft.rfftfreq(N, dt)
    if f.size <= 1:
        return np.nan
    mag = np.abs(Z)
    mag[0] = 0.0  # ignore DC
    # band: at least ~3 cycles in window and <= 90% Nyquist
    fmin = 3.0 / (N * dt)
    fmax = 0.45 / dt
    band = (f >= fmin) & (f <= fmax)
    if not np.any(band):
        return np.nan
    k = np.argmax(mag[band])
    idx = np.flatnonzero(band)[k]
    f0 = float(f[idx])
    return np.nan if f0 <= 0 else (1.0 / f0)

def _estimate_period_acf(y: np.ndarray, dt: float) -> float:
    """Estimate dominant period via autocorrelation peak; NaN if not reliable."""
    N = y.size
    if N < 64 or not np.isfinite(dt) or dt <= 0:
        return np.nan
    z = y - np.mean(y)
    if np.allclose(z, 0.0):
        return np.nan
    # FFT-based autocorrelation
    L = int(1 << (N - 1).bit_length())  # next pow2 for speed
    Z = np.fft.rfft(z, n=2 * L)
    ac = np.fft.irfft(np.abs(Z) ** 2)[:N]
    ac = ac / max(ac[0], 1e-12)
    # search lag range: 3 cycles min within window -> lag <= N/3
    min_lag = 3
    max_lag = N // 3
    if max_lag <= min_lag:
        return np.nan
    lag = min_lag + int(np.argmax(ac[min_lag:max_lag]))
    if lag <= 0:
        return np.nan
    return lag * dt

def _best_period(y: np.ndarray, dt: float) -> float:
    T_fft = _estimate_period_fft(y, dt)
    if np.isfinite(T_fft):
        return T_fft
    return _estimate_period_acf(y, dt)

# --------------------------- Template construction -------------------------- #

def _align_and_average(y: np.ndarray, spp: int, max_periods: int = 12, template_points: int = 128) -> np.ndarray:
    """
    Build a normalized template by aligning up to max_periods cycles (length spp),
    using circular shift to maximize correlation with the first cycle.
    """
    n = y.size
    spp = int(max(8, spp))
    K = min(max_periods, n // spp)
    if K < 2:
        return np.array([], dtype=float)

    ref = y[0:spp].astype(float)
    ref = ref - np.mean(ref)
    s = float(np.std(ref))
    if s > 0:
        ref /= s

    acc = np.zeros(spp, dtype=float)
    cnt = 0
    for i in range(K):
        seg = y[i * spp:(i + 1) * spp].astype(float)
        if seg.size != spp:
            break
        seg = seg - np.mean(seg)
        std = float(np.std(seg))
        if std > 0:
            seg = seg / std
        # align by circular shift maximizing correlation
        c = np.fft.ifft(np.fft.fft(ref) * np.conj(np.fft.fft(seg))).real
        shift = int(np.argmax(c)) % spp
        seg_aligned = np.roll(seg, shift)
        acc += seg_aligned
        cnt += 1

    if cnt == 0:
        return np.array([], dtype=float)

    avg = acc / cnt
    # resample to a fixed phase grid for stable comparison
    xi = np.linspace(0, 1, spp, endpoint=False)
    phase_grid = np.linspace(0, 1, template_points, endpoint=False)
    tpl = np.interp(phase_grid, xi, avg)
    # unit norm
    norm = float(np.linalg.norm(tpl))
    if norm > 0:
        tpl = tpl / norm
    return tpl

def _window_correlation(y: np.ndarray, i: int, spp: int, template: np.ndarray) -> float:
    """Normalized correlation of one window (length spp) to template (phase grid)."""
    seg = y[i:i + spp]
    if seg.size != spp:
        return np.nan
    seg = seg - np.mean(seg)
    std = float(np.std(seg))
    if std == 0:
        return 0.0
    seg = seg / std
    # resample to template length
    xi = np.linspace(0, 1, seg.size, endpoint=False)
    seg_t = np.interp(np.linspace(0, 1, template.size, endpoint=False), xi, seg)
    denom = float(np.linalg.norm(seg_t))
    if denom == 0:
        return 0.0
    return float(np.dot(seg_t, template) / denom)

# ------------------------------ Main detector ------------------------------- #

def pattern_fault_time(signal_view) -> float:
    """
    Return the first timestamp (float seconds) where the auto-learned repeating pattern
    is disturbed and stays disturbed briefly. Returns float('nan') if not detected.
    """
    x, y = _as_numpy_xy(signal_view)
    if x.size < 128:
        return float("nan")

    # Uniform resample
    tu, yu, dt = _uniform_resample(x, y)
    if not np.isfinite(dt) or tu.size < 128:
        return float("nan")

    # Learn window
    learn_fraction = 0.30
    N = tu.size
    N_learn = max(128, int(N * learn_fraction))
    N_learn = min(N_learn, max(128, N // 2))
    yL = yu[:N_learn]

    # Period estimate
    T = _best_period(yL, dt)
    if not np.isfinite(T):
        return float("nan")
    spp = int(round(T / dt))
    if spp < 8:
        return float("nan")
    if spp > N_learn // 2:
        # Estimated period too long for reliable learning
        return float("nan")

    # Template
    template = _align_and_average(yL, spp, max_periods=12, template_points=128)
    if template.size == 0:
        return float("nan")

    # Sliding correlation (tolerant)
    hop = max(1, int(round(spp / 8)))               # ~8 hops per period
    hold_windows = max(1, int(round(0.1 * spp / hop)))  # ~10% of a period
    corrs, times, means = [], [], []
    for i in range(0, N - spp, hop):
        c = _window_correlation(yu, i, spp, template)
        corrs.append(c)
        means.append(float(np.mean(yu[i:i + spp])))
        times.append(tu[i + spp // 2])
    corrs = np.asarray(corrs, dtype=float)
    means = np.asarray(means, dtype=float)
    times = np.asarray(times, dtype=float)
    if times.size == 0:
        return float("nan")

    # Baseline stats from learn region
    learn_mask = times <= tu[min(N_learn - 1, N - 1)]
    if not np.any(learn_mask):
        return float("nan")
    base_corr = corrs[learn_mask]
    base_mean = means[learn_mask]

    mc, sc = _robust_mean_std(base_corr)
    mm, sm = _robust_mean_std(base_mean)
    if not np.isfinite(mc) or not np.isfinite(sc) or not np.isfinite(mm) or not np.isfinite(sm):
        return float("nan")

    # Adaptive thresholds
    corr_floor = max(0.3, mc - 2.5 * sc, 0.85 * mc)      # relative drop
    mean_dev_thr = max(3.0 * sm, 0.01 * max(1.0, abs(mm)))  # scale-aware

    # Start scanning after learning window
    j0 = int(np.searchsorted(times, tu[min(N_learn - 1, N - 1)], side="right"))
    bad_corr = corrs < corr_floor
    bad_mean = np.abs(means - mm) > mean_dev_thr

    # Require sustained violation by either criterion
    run = 0
    for j in range(j0, times.size):
        if bad_corr[j] or bad_mean[j]:
            run += 1
            if run >= hold_windows:
                # return time of first disturbed window, not delayed
                return float(times[j - run + 1])
        else:
            run = 0

    return float("nan")

__all__ = ["pattern_fault_time"]
